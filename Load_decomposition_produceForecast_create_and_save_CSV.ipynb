{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sparkContext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fbf9ccaeb39c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sparkContext' is not defined"
     ]
    }
   ],
   "source": [
    "sSparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-57c8e1770bd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mforecast\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m      \u001b[0mdecomp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickleFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbacktest_year\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# Load pickle files with saved decompositions \n",
    "# set avergagin method. options are 'mean' 'median' 'exp'\n",
    "\n",
    "\n",
    "forecast = True\n",
    "backtest_year  = 2014\n",
    "\n",
    "path = {}\n",
    "decomp = {}\n",
    "\n",
    "year_list = [2013,2014]\n",
    "the34_23 = '2_3'\n",
    "\n",
    "num_of_stores = 800\n",
    "\n",
    "for y in year_list:\n",
    "    path[y] = 'data/800_2_3_xmastrial'\n",
    "#path[y] = 'data/800_2_3_xmastrial'\n",
    "\n",
    "if forecast:\n",
    "     decomp[y] =  dict(sc.pickleFile(path[y]).collect())\n",
    "else:\n",
    "    if y < backtest_year:\n",
    "        decomp[y] =  dict(sc.pickleFile(path[y]).collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print np.where(decomp[2013]['F11W']['stores'] == 3137 )\n",
    "#print np.where(decomp[2013]['F11W']['products'] == 'F11WE')\n",
    "\n",
    "#decomp[2013]['F11W']['E_T'][411,:,3]\n",
    "#decomp[2013]['F11W']['E_T']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the PTPs\n",
    "\n",
    "if forecast:\n",
    "        PTPs_path  = 'zg43_xmasPTPs'\n",
    "else:\n",
    "        PTPs_path  = 'zg43_xmasPTPs_'+str(backtest_year)\n",
    "\n",
    "    \n",
    "xmas_ptps = (sqlContext.read.format('jdbc')\n",
    "                .options(url=\"jdbc:teradata://TDPS.ukroi.tesco.org/DATABASE=DXWI_PROD_CRORDER_PLAY_PEN\",\n",
    "                     dbtable=PTPs_path,\n",
    "                     user = \"GCT8\",\n",
    "                     password = \"ludo4ka\",\n",
    "                     driver = \"com.teradata.jdbc.TeraDriver\")\n",
    "                .load()\n",
    "             )\n",
    "    \n",
    "    \n",
    "#xmas_ptps = (sqlContext.read.format(\"com.databricks.spark.csv\")\n",
    "#                .option(\"header\",\"false\")\n",
    "#                .option(\"inferSchema\", \"true\")\n",
    "#                .load(PTPs_path)\n",
    "#                .toDF('retail_outlet_number','product','calendar_date','ptp')\n",
    "#                )\n",
    "\n",
    "\n",
    "\n",
    "xmas_ptps  = (xmas_ptps\n",
    "                  .select(substring(xmas_ptps.Product_Sub_Group_Code,0,4).alias('group'),'*')\n",
    "                  .withColumnRenamed(\"Product_Sub_Group_Code\",\"product\")\n",
    "                  .withColumn('ptp',xmas_ptps['ptp'].cast(DoubleType()))\n",
    "                  .orderBy(asc(\"Calendar_Date\"))\n",
    "                 )\n",
    "\n",
    "print xmas_ptps.count()\n",
    "print xmas_ptps.printSchema\n",
    "xmas_ptps_pandas = xmas_ptps.toPandas()\n",
    "\n",
    "#xmas_ptps_pandas.hierarchy_group.str.strip()\n",
    "#bonfireNightPTPs_pandas\n",
    "#nSubgrp = bonfireNightPTPs_pandas\n",
    "print xmas_ptps_pandas.shape\n",
    "xmas_ptps_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmas_ptps_pandas['group'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "years_vec = decomp.keys()\n",
    "method = 'median'\n",
    "\n",
    "#% years_vec = [2012 2013 2014 ];\n",
    "plot_figs = False #% 0 to disable plotting\n",
    "#% a temporary and random file load, to extract some parameters\n",
    "\n",
    "group_names = decomp[years_vec[0]].keys()\n",
    "\n",
    "events  = decomp[years_vec[0]][group_names[0]]['events']\n",
    "\n",
    "# collect all subgroups names\n",
    "subgroups_names = list()\n",
    "for  g in group_names:\n",
    "            subgroups_names =subgroups_names + list(decomp[years_vec[0]][g]['products'])\n",
    "\n",
    "stores =  decomp[years_vec[0]][group_names[0]]['stores']\n",
    "nStores =len(stores)\n",
    "dateVec =  decomp[years_vec[0]][group_names[0]]['dateVec']\n",
    "print len(subgroups_names)\n",
    "print len(dateVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nSubgrp = len(subgroups_names) \n",
    "nDays = decomp[years_vec[0]][decomp[years_vec[0]].keys()[0]]['E_BV'].shape[1]\n",
    "\n",
    "# initializing store_day_subgr_yr_tensor\n",
    "store_day_subgr_yr_event_tensor = np.zeros((nStores,nDays,nSubgrp,len(years_vec)))\n",
    "store_day_subgr_yr_basis_tensor = np.zeros((nStores,nDays,nSubgrp,len(years_vec)))\n",
    "store_day_subgr_yr_ptp_tensor   = np.zeros((nStores,nDays,nSubgrp,len(years_vec)))\n",
    "storeRange = np.zeros((nStores, nSubgrp, len(years_vec)))\n",
    "\n",
    "\n",
    "# looping over all subgroups\n",
    "subgroup_ind = 0\n",
    "for subgroup in subgroups_names:\n",
    "    yr_ind = 0\n",
    "    print subgroup\n",
    "    for y  in years_vec:\n",
    "        print y\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        #% Constructing the filename\n",
    "        #filename = ['bonfireSubgroupDecomps', subgroups_names{subgroup_ind}, num2str(years_vec(yr_ind)), '.mat\n",
    "        group_name = subgroup[:-1]\n",
    "        dat = decomp[y][group_name]\n",
    "        \n",
    "        jj = np.where(decomp[y][group_name]['products'] == subgroup)[0]\n",
    "        # Sales matrix (stores, days)\n",
    "        \n",
    "        X = dat['X'][:,:,jj]\n",
    "        X = X.reshape((X.shape[0],X.shape[1]))\n",
    "        #results = dat['results']\n",
    "        T = dat['E_T'][:,:,jj]\n",
    "        T  = T.reshape((T.shape[0],T.shape[1]))\n",
    "        \n",
    "        V = dat['E_V'][:,:,jj]\n",
    "        V =  V.reshape((V.shape[0],V.shape[1]))\n",
    "        \n",
    "        N = dat['N'][:,:,jj]\n",
    "        N =  N.reshape((N.shape[0],N.shape[1]))\n",
    "        \n",
    "        ptp = dat['ptp'][:,:,jj]\n",
    "        ptp =  ptp.reshape((ptp.shape[0],ptp.shape[1]))\n",
    "        \n",
    "        \n",
    "        #%% forming the base shapes\n",
    "        n_basis = dat['basis_m'].shape[0]\n",
    "        \n",
    "        basis_inds_vec = range( 0, n_basis)\n",
    "        T_basis = T[:,basis_inds_vec]\n",
    "        V_basis = V[basis_inds_vec,:]\n",
    "        basis_time = N * np.dot(T_basis , V_basis)\n",
    "        \n",
    "        # forming the promos\n",
    "        n_promos = dat['promos'].shape[0]\n",
    "        promos_inds_vec = range(n_basis, n_basis+n_promos)\n",
    "        T_promos = T[:,promos_inds_vec]\n",
    "        V_promos = V[promos_inds_vec,:]\n",
    "        promos_time = N * np.dot(T_promos , V_promos)\n",
    "        \n",
    "        # forming the events\n",
    "        events_inds_vec = range(n_basis+n_promos, V.shape[0] )\n",
    "        T_events = T[:,events_inds_vec]\n",
    "        V_events = V[events_inds_vec,:]\n",
    "        events_time = N * np.dot(T_events , V_events)\n",
    "        \n",
    "        #%% including ptp\n",
    "        X_hat = ptp * (basis_time + promos_time + events_time)\n",
    "        \n",
    "        #%% setting X-axis as date\n",
    "                           \n",
    "        #from datetime import datetime\n",
    "\n",
    "        #date_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n",
    "        #date_1 = datetime.datetime.strptime(start_date, \"%m/%d/%y\")\n",
    "        #end_date = date_1 + datetime.timedelta(days=10)\n",
    "        #all_days = datetime.strptime(dat['startDate']) + datetime.timedelta(days=dat['nDays'])\n",
    "        #all_days =                    \n",
    "        #dat['endDate'] = all_days[-1] # % had disappeared, recalc'd\n",
    "        #xData = linspace(dat.startDate,dat.endDate,size(X,2));\n",
    "        \n",
    "        #import datetime\n",
    "        #all_days = range(datetime.datetime.strptime('2015-03-03',\"%Y-%m-%d\"),datetime.datetime.strptime('2015-03-03',\"%Y-%m-%d\") + datetime.timedelta(days=100))\n",
    "        \n",
    "        #all_days = [datetime.datetime.strptime('2015-03-03',\"%Y-%m-%d\") + datetime.timedelta(days=x) for x in range(0, dat['nDays'])]\n",
    "\n",
    "        #xData = linspace(dat.startDate,dat.endDate,size(X,2)) # not sure what should it be replaced with\n",
    "    \n",
    "    \n",
    "        store_day_subgr_yr_event_tensor[:,:,subgroup_ind,yr_ind] = events_time\n",
    "        store_day_subgr_yr_basis_tensor[:,:,subgroup_ind,yr_ind] = basis_time\n",
    "        store_day_subgr_yr_basis_tensor[:,:,subgroup_ind,yr_ind] = promos_time\n",
    "        store_day_subgr_yr_ptp_tensor[:,:,subgroup_ind,yr_ind] = ptp\n",
    "        \n",
    "        if subgroup_ind == 0:\n",
    "            storeRange_bases = np.zeros(( nStores, len(subgroups_names ), len(years_vec)))\n",
    "            storeRange_events  = np.zeros(( nStores, len(subgroups_names ), len(years_vec)))\n",
    "      \n",
    "        storeRange_bases[:,subgroup_ind,yr_ind]  = np.all(store_day_subgr_yr_basis_tensor[:,:,subgroup_ind,yr_ind]==0, 1)\n",
    "        storeRange_events[:,subgroup_ind,yr_ind] = np.all(store_day_subgr_yr_event_tensor[:,:,subgroup_ind,yr_ind]==0, 1)\n",
    "\n",
    "        #% minuture version\n",
    "     \n",
    "        basis_store_days = store_day_subgr_yr_basis_tensor[:,:,subgroup_ind,yr_ind]\n",
    "        event_store_days = store_day_subgr_yr_event_tensor[:,:,subgroup_ind,yr_ind]\n",
    "\n",
    "\n",
    "        #% calculating nan years for all subgroups\n",
    "        if subgroup_ind == 0:\n",
    "            nanSubgroupYear_bases  = np.zeros(( len(subgroups_names ), len(years_vec)))\n",
    "            nanSubgroupYear_events =  np.zeros(( len(subgroups_names ), len(years_vec)))\n",
    "            \n",
    "        nanSubgroupYear_bases[subgroup_ind,yr_ind] = np.sum(np.isnan(basis_store_days))  == basis_store_days.shape[0] * basis_store_days.shape[1]\n",
    "        nanSubgroupYear_events[subgroup_ind,yr_ind] = np.sum(np.isnan(event_store_days)) == event_store_days.shape[0] * event_store_days.shape[1]\n",
    "\n",
    "\n",
    "        #if y == 2015:\n",
    "        #    if subgroup_ind == 0:\n",
    "        #        salesLastYear  = np.zeros((X_hat.shape[0],X_hat.shape[1], len(subgroups_names )))\n",
    "        #        salesLastYear[:,:,subgroup_ind] = X_hat\n",
    "        #    else:\n",
    "        #        salesLastYear[:,:,subgroup_ind] = X_hat\n",
    "        \n",
    "        yr_ind = yr_ind + 1\n",
    "                \n",
    "    subgroup_ind = subgroup_ind + 1\n",
    "      \n",
    "#nYearsMissing_bases = np.sum(storeRange_bases,3)\n",
    "#[i_b,j_b] = np.where(nYearsMissing_bases == len(years_vec))\n",
    "                                  \n",
    "#list_of_allZeroYears_store_subgrp_fromBases = [i_b, j_b]\n",
    "\n",
    "#nYearsMissing_events = np.sum(storeRange_events,3);\n",
    "#[i_e,j_e] = np.where(nYearsMissing_events == len(years_vec))\n",
    "                                  \n",
    "#list_of_allZeroYears_store_subgrp_fromEvents = [i_e j_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% initialize\n",
    "#% nan_sg_yr = cell(1,nSubgrp);\n",
    "event_subgroups = {}\n",
    "basis_subgroups = {}\n",
    "#% ptp_subgroups   = {cell(1,nSubgrp);}\n",
    "\n",
    "#% seperate subgroups\n",
    "subgroup_ind = 0\n",
    "for subgroup in subgroups_names:\n",
    "        #%nan_sg_yr{subgroup_ind} = squeeze(any(any(isnan(store_day_subgr_yr_basis_tensor(:,:,subgroup_ind,:)))));\n",
    "        event_subgroups[subgroup] = np.squeeze(store_day_subgr_yr_event_tensor[:,:,subgroup_ind,:])\n",
    "        basis_subgroups[subgroup] = np.squeeze(store_day_subgr_yr_basis_tensor[:,:,subgroup_ind,:])\n",
    "        #%ptp_subgroups{subgroup_ind} = squeeze(store_day_subgr_yr_basis_tensor(:,:,subgroup_ind,:));\n",
    "        subgroup_ind = subgroup_ind + 1\n",
    "\n",
    "\n",
    "#%% prediction\n",
    "#% initilize\n",
    "event_pred_subgr_mat = {}  # % contains the predicted (in this case 'mean') event values\n",
    "basis_pred_subgr_mat = {} # % contains the predicted (in this case 'mean') basis values\n",
    "#%basis_pred_subgr_mat_no_ptp = cell(1, nSubgrp); % contains the predicted (in this case 'mean') basis values\n",
    "\n",
    "\n",
    "#% calculate the predictions\n",
    "\n",
    "\n",
    "for subgroup in subgroups_names:       \n",
    "        if len(event_subgroups[subgroup].shape) == 2:\n",
    "            event_pred_subgr_mat[subgroup]        =  event_subgroups[subgroup] \n",
    "            basis_pred_subgr_mat[subgroup]        =  basis_subgroups[subgroup] \n",
    "           \n",
    "        else:  \n",
    "        \n",
    "            if method == 'mean':\n",
    "                event_pred_subgr_mat[subgroup]        =  np.mean(event_subgroups[subgroup],2) \n",
    "                basis_pred_subgr_mat[subgroup]        =  np.mean(basis_subgroups[subgroup],2) \n",
    "            elif method == 'median':\n",
    "                #pdb.set_trace()\n",
    "                event_pred_subgr_mat[subgroup]        =  np.median(event_subgroups[subgroup],2) \n",
    "                basis_pred_subgr_mat[subgroup]        =  np.median(basis_subgroups[subgroup],2) \n",
    "            elif method == 'exp':\n",
    "                alpha =  0.5\n",
    "                w = exp_smoothing_average_weights(alpha,event_subgroups[subgroup].shape[2])\n",
    "        \n",
    "                event_pred_subgr_mat[subgroup]        =  np.average(event_subgroups[subgroup],2,weights= w) \n",
    "                basis_pred_subgr_mat[subgroup]        =  np.average(basis_subgroups[subgroup],2, weights= w) \n",
    "            else:\n",
    "                print 'incorrect method is set'\n",
    "        \n",
    "        \n",
    "\n",
    "#%     event_pred_subgr_mat{subgr_}        = mean(ptp_subgroups{subgroup_ind} .* event_subgroups{subgr_},3);\n",
    "#%     basis_pred_subgr_mat{subgr_}        = mean(ptp_subgroups{subgroup_ind} .* basis_subgroups{subgr_},3);\n",
    "#    %basis_pred_subgr_mat_no_ptp{subgr_} = mean(basis_subgroups{subgr_},3);\n",
    "\n",
    "#% Exporting predicted event and basis function shapes for the APP\n",
    "\n",
    "#% Replace NaNs with 0\n",
    "for subgroup in subgroups_names:\n",
    "        event_pred_subgr_mat[subgroup][np.isnan(event_pred_subgr_mat[subgroup])] = 0\n",
    "        basis_pred_subgr_mat[subgroup][np.isnan(basis_pred_subgr_mat[subgroup])] = 0\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#exportDecomps draft\n",
    "# generate CSV files\n",
    "trialStores = xmas_ptps_pandas['Retail_Outlet_Number'].unique() # hardcode\n",
    "\n",
    "unionTable = False\n",
    "\n",
    "   \n",
    "if forecast:\n",
    "    forecast_date_start = '2016-11-28'\n",
    "    forecast_date_end = '2017-01-15'\n",
    "else:\n",
    "    backtest_year\n",
    "    forecast_date_start = str(backtest_year)+'-11-28'\n",
    "    forecast_date_end = str(backtest_year+1)+'-01-15'\n",
    "\n",
    "delta =  ((datetime.datetime.strptime(forecast_date_end ,\"%Y-%m-%d\")) -  \n",
    "                    (datetime.datetime.strptime(forecast_date_start ,\"%Y-%m-%d\"))).days\n",
    "\n",
    "\n",
    "\n",
    "date_range_index  =(( np.array(dateVec) >= datetime.datetime.strptime(str(dateVec[0].year)+'-11-28', \"%Y-%m-%d\") ) \n",
    "                            * (np.array(dateVec) <= datetime.datetime.strptime(str(dateVec[0].year+1)+'-01-15', \"%Y-%m-%d\")))\n",
    "    \n",
    "print np.sum(date_range_index)\n",
    "#ptp_filename =  'D:\\projects\\forecasting\\bonfireNightPTPs.csv';\n",
    "#ptp_table = import_bonfireNightPTPs(ptp_filename);\n",
    "#ptp_table.Product_Sub_Group_Code = strtrim(ptp_table.Product_Sub_Group_Code);\n",
    "\n",
    "PTP_pandas = xmas_ptps_pandas\n",
    "\n",
    "#print bonfireNightPTPs\n",
    "#bonfireNightPTPs_df.show()\n",
    "\n",
    "#bonfireNightPTPs_pandas.hierarchy_group.str.strip()\n",
    "\n",
    "counter_hg =  0\n",
    "\n",
    "dateVec_F = [(datetime.datetime.strptime(forecast_date_start ,\"%Y-%m-%d\")\n",
    "                        + datetime.timedelta(days=x)).strftime('%Y-%m-%d') for x in range( delta +1)\n",
    "          ]\n",
    "       \n",
    "\n",
    "#dateVecLY = [(datetime.datetime.strptime(forecast_date_start ,\"%Y-%m-%d\")\n",
    "#                        + datetime.timedelta(days=x)).strftime('%Y-%m-%d') for x in range( delta  +1)\n",
    "#            ]\n",
    "      \n",
    "for hg in xmas_ptps_pandas['product'].unique():\n",
    "    \n",
    "    print 'Loading subgroup '+hg \n",
    "\n",
    "    for st in trialStores:\n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        ptp2016 = PTP_pandas[PTP_pandas['Retail_Outlet_Number'] == st][PTP_pandas['product'] == hg]['ptp']\n",
    "    \n",
    "        baseSalesFcast     = ptp2016.T * basis_pred_subgr_mat[hg][ stores == st, date_range_index ]\n",
    "        eventForecast      = ptp2016.T * event_pred_subgr_mat[hg][ stores == st, date_range_index ]\n",
    "        \n",
    "        baseSalesFcast[np.isnan(baseSalesFcast)] = 0\n",
    "        eventForecast[np.isnan(eventForecast)] = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        xmasForecast          = np.zeros((eventForecast.shape))\n",
    "        #halloweenForecast        = np.zeros((eventForecast.shape))\n",
    "        xmasForecast[events[2,date_range_index] == 1  ] = eventForecast[events[2,date_range_index] == 1 ]\n",
    "        #halloweenForecast[0: 5 ]  = eventForecast[0: 5 ]\n",
    "        \n",
    "        #ptp2015 = np.vstack( ptp2016[5:-1],  ptp2016[0:5])\n",
    "        \n",
    "        # not sure what is meant here -  will need to coordinate with Bobby to proceed\n",
    "        #salesLY = ptp2015.T * salesLastYear[ stores == st, forecast_date_range, counter_hg]\n",
    "        #salesLY[np.isnan(salesLY)] = 0\n",
    "        \n",
    "        baseTable = pd.DataFrame(np.hstack( (\n",
    "                                          np.tile(st, (delta + 1,1)),\n",
    "                                          np.tile(hg,(delta + 1,1)),\n",
    "                                          np.zeros((delta + 1,1)),\n",
    "                                          np.array(dateVec_F).reshape(len(dateVec_F),1),\n",
    "                                          baseSalesFcast.T.reshape((baseSalesFcast.T.shape[0],1)),\n",
    "                                          np.tile('?',(delta + 1, 1)),\n",
    "                                          np.zeros((delta + 1, 1)),\n",
    "                                          np.tile('?',(delta + 1, 1)),\n",
    "                                          np.tile('?',(delta + 1, 1))) ),\n",
    "                                          columns = [ 'Store','hierarchy_code','zero1','calendar_date',\n",
    "                                                    'forecast','qMark1','zero2','qMark2','qMark3'])\n",
    "        \n",
    "        \n",
    "        \n",
    "        xmasTable = pd.DataFrame(np.hstack( (\n",
    "                                        np.tile(5,( delta + 1, 1)),\n",
    "                                        np.tile(st,( delta + 1, 1)),\n",
    "                                        np.tile(hg, (delta + 1, 1)),\n",
    "                                        np.zeros((delta + 1, 1)),\n",
    "                                        np.array(dateVec_F).reshape(len(dateVec_F),1),\n",
    "                                        xmasForecast.T.reshape((xmasForecast.T.shape[0],1)),\n",
    "                                        np.tile('?',(delta + 1, 1)),\n",
    "                                        np.zeros((delta + 1,1)),\n",
    "                                        np.tile('?',(delta + 1, 1)),\n",
    "                                        np.tile('?',(delta + 1, 1)) ) ),\n",
    "                                        columns =  [ 'Event_id', 'store','hierarchy_code',\n",
    "                                                    'zero1','calendar_date','forecast','qMark1','zero2','qMark2','qMark3'])\n",
    "        \n",
    "#        halloweenTable = pd.DataFrame(np.hstack(\n",
    "#                                        np.tile(4, (delta + 1, 1)),\n",
    "#                                        np.tile(st, (delta + 1, 1)),\n",
    "#                                        np.tile(hg, (delta + 1, 1)),\n",
    "#                                        np.zeros((delta + 1, 1)),\n",
    "#                                        dateVec,\n",
    "#                                        halloweenForecast.T,\n",
    "#                                        np.tile('?',(delta + 1, 1)),\n",
    "#                                        np.zeros((delta + 1,1)),\n",
    "#                                        np.tile('?',(delta + 1, 1)),\n",
    "#                                        np.tile('?',(delta + 1, 1)),\n",
    "#                                        columns =  [ 'Event_id', 'store','hierarchy_code',\n",
    "#                                                    'zero1','date','forecast','qMark1','zero2','qMark2','qMark3']))\n",
    "        \n",
    "        #eventsTable = union(bonfireTable, halloweenTable);\n",
    "        #eventsTable = bonfireTable.append(halloweenTable, ignore_index=True)\n",
    "        \n",
    "        #LYTable = pd.DataFrame(np.hstack(\n",
    "        #                            np.tile(st, (delta + 1,1)),\n",
    "        #                            np.tile(hg, (delta + 1,1)),\n",
    "        #                            np.zeros((delta + 1,1)),\n",
    "        #                            dateVecLY,\n",
    "        #                            salesLY.T,\n",
    "        #                            columns = [ 'Store','hierarchy_code','zero','date','sales']))\n",
    "        \n",
    "        \n",
    "        if  unionTable:\n",
    "            baseTableOut = baseTable.append(baseTableOut, ignore_index=True)\n",
    "            xmasTableOut = xmasTable.append(xmasTableOut, ignore_index=True)\n",
    "        #    LYTableOut = LYTable.append(LYTableOut, ignore_index=True )\n",
    "        else:\n",
    "            baseTableOut = baseTable\n",
    "            xmasTableOut = xmasTable\n",
    "        #    LYTableOut = LYTable\n",
    "            \n",
    "            unionTable = True\n",
    "\n",
    "        #             dcmp.utils.bnmf_diagnostics(results, X./results.ptp, X, zeros(size(X)), zeros(nStores,np), ones(np, nDays), 1:nBasis+1);\n",
    " \n",
    "    counter_hg = counter_hg + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-f9ef205a8f26>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-f9ef205a8f26>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    stop here\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tempOutputTable = 'zg43_test_bonfire_data';\n",
    "\n",
    "xmas_baseTable_filename = ('/insight_labs/retaildemandforecasting/data/xmas/output/'\n",
    "                           +str(num_of_stores)+\n",
    "                           '_'+the34_23\n",
    "                           +'_xmasBaseTableCSV_2111_2015'\n",
    "                           +('forecast' if forecast else 'backtest')\n",
    "                            +'_'\n",
    "                            +method\n",
    "                          )\n",
    "\n",
    "xmas_eventTable_filename = ('/insight_labs/retaildemandforecasting/data/xmas/output/'\n",
    "                            +str(num_of_stores)\n",
    "                            +'_'+the34_23\n",
    "                            +'_xmasEventsTableCSV_2111_2015'\n",
    "                            +('forecast' if forecast else 'backtest')\n",
    "                            +'_'\n",
    "                            +method)\n",
    "#bonfire_LYTable_filename = 'bonfireLYTableCSV.txt';\n",
    "#bonfire_baseTable_pipe_filename = 'bonfireBaseTableCSV_pipe.txt';\n",
    "#bonfire_eventTable_pipe_filename = 'bonfireEventsTableCSV_pipe.txt';\n",
    "#bonfire_LYTable_pipe_filename = 'bonfireLYTableCSV_pipe.txt';\n",
    "#formatOut = 'dd_mm_yyyy__HH_MM';\n",
    "#time_str=datestr(now,formatOut);\n",
    "#folder_name = ['bonfire_',time_str];\n",
    "#mkdir(folder_name)\n",
    "#cd(folder_name)\n",
    "\n",
    "#% writetable(baseTableOut,fullfile(dataRoot,bonfire_baseTable_filename));\n",
    "#% writetable(eventsTableOut,fullfile(dataRoot,bonfire_eventTable_filename));\n",
    "#% writetable(LYTableOut,fullfile(dataRoot,bonfire_LYTable_filename));\n",
    "\n",
    "(sqlContext\n",
    "        .createDataFrame(baseTableOut)\n",
    "        .coalesce(1)\n",
    "        .write\n",
    "        .format(\"com.databricks.spark.csv\")\n",
    "        .option(\"header\",\"false\")\n",
    "        .save(xmas_baseTable_filename) )\n",
    "        \n",
    "(sqlContext\n",
    "        .createDataFrame(xmasTableOut)\n",
    "        .coalesce(1)\n",
    "        .write\n",
    "        .format(\"com.databricks.spark.csv\")\n",
    "        .option(\"header\",\"false\")\n",
    "        .save(xmas_eventTable_filename) )\n",
    "        \n",
    "        \n",
    "#(sqlContext\n",
    "#        .createDataFrame(LYTableOut)\n",
    "#        .coalesce(1)\n",
    "#        .write\n",
    "#        .format(\"com.databricks.spark.csv\")\n",
    "#        .option(\"header\",\"true\")\n",
    "#        .save(bonfire_LYTable_filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "xmasTableOut_df = (sqlContext\n",
    "                     .createDataFrame(xmasTableOut)\n",
    "                   )\n",
    "\n",
    "xmasTableOut_df = xmasTableOut_df.withColumn(\"Event_id\", xmasTableOut_df[\"Event_id\"].cast(IntegerType()))\n",
    "xmasTableOut_df = xmasTableOut_df.withColumn(\"store\", xmasTableOut_df[\"store\"].cast(IntegerType()))\n",
    "xmasTableOut_df = xmasTableOut_df.withColumn(\"zero1\", xmasTableOut_df[\"zero1\"].cast(DoubleType()))\n",
    "xmasTableOut_df = xmasTableOut_df.withColumn(\"calendar_date\", xmasTableOut_df[\"calendar_date\"].cast(DateType()))\n",
    "xmasTableOut_df = xmasTableOut_df.withColumn(\"forecast\", xmasTableOut_df[\"forecast\"].cast(DoubleType()))\n",
    "xmasTableOut_df = xmasTableOut_df.withColumn(\"zero2\", xmasTableOut_df[\"zero2\"].cast(DoubleType()))\n",
    "xmasTableOut_df = xmasTableOut_df.withColumn(\"hierarchy_code\", xmasTableOut_df[\"hierarchy_code\"].cast(StringType(\"varchar\")))\n",
    "\n",
    "xmasTableOut_df = xmasTableOut_df.withColumn(\"rowId_for_primary_key\", monotonically_increasing_id()) \n",
    "\n",
    "\n",
    "                                                                  \n",
    "xmasTableOut_df.cache()\n",
    "\n",
    "xmasTableOut_df.printSchema()\n",
    "xmasTableOut_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets test writign into TRD table \n",
    "\n",
    "prop = {\n",
    "    \"user\": \"GCT8\",\n",
    "    \"password\": \"ludo4ka\",\n",
    "    \"driver\" : \"com.teradata.jdbc.TeraDriver\"\n",
    "}\n",
    "\n",
    "(xmasTableOut_df.select(xmasTableOut_df.rowId_for_primary_key,\n",
    "                        xmasTableOut_df.Event_id,\n",
    "                        xmasTableOut_df.store,\n",
    "                        xmasTableOut_df.zero1,\n",
    "                        xmasTableOut_df.calendar_date,\n",
    "                        xmasTableOut_df.forecast,\n",
    "                        xmasTableOut_df.qMark1,\n",
    "                        xmasTableOut_df.zero2\n",
    "                        \n",
    "                        \n",
    "                       )\n",
    "                 .write\n",
    "                 .jdbc(url  = \"jdbc:teradata://TDPS.ukroi.tesco.org/DATABASE=DXWI_PROD_CRORDER_PLAY_PEN\",\n",
    "                       table = \"xmas_Events_2111\",\n",
    "                       mode = \"overwrite\",\n",
    "                       properties  = prop\n",
    "                      )\n",
    ")\n",
    "      \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sqopp way\n",
    "!sqoop export --connect jdbc:teradata://TDPS.ukroi.tesco.org/DATABASE=DXWI_PROD_CRORDER_PLAY_PEN --username GCT8 --password ludo4ka --table xmas_BackTestEvents_2111_1 --export-dir /insight_labs/retaildemandforecasting/data/xmas/output/800_2_3_xmasEventsTableCSV_2111_2015forecast_median  --input-fields-terminated-by ',' --lines-terminated-by '\\n' --m 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets test writign into TRD table \n",
    "\n",
    "(sqlContext\n",
    "                .createDataFrame(xmasTableOut)\n",
    "                .coalesce(1)\n",
    "                .write\n",
    "                .format('jdbc')\n",
    "                .options(url=\"jdbc:teradata://TDPS.ukroi.tesco.org/DATABASE=DXWI_PROD_CRORDER_PLAY_PEN\",\n",
    "                     dbtable=\"xmas_BackTestEvents_2111\",\n",
    "                     user = \"GCT8\",\n",
    "                     password = \"ludo4ka\",\n",
    "                     driver = \"com.teradata.jdbc.TeraDriver\")\n",
    "                .save()\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (sqlContext.read.format('jdbc')\n",
    "        .options(url=\"jdbc:teradata://TDPS.ukroi.tesco.org/DATABASE=DXWI_PROD_CRORDER_PLAY_PEN\",\n",
    "                 dbtable=\"zg43_xmasPTPs\",\n",
    "                 user = \"GCT8\",\n",
    "                 password = \"ludo4ka\",\n",
    "                 driver = \"com.teradata.jdbc.TeraDriver\")\n",
    "        .load()\n",
    "     )\n",
    "\n",
    "\n",
    "\n",
    "#jdbcDF = sqlContext.read.format(\"jdbc\").options(\"url\" = \"TDPS.ukroi.tesco.org/DATABASE=DXWI_PROD_CRORDER_PLAY_PEN\",\n",
    "#   \"user\" -> \"GCT8\",\n",
    "#   \"password\" -> \"ludo4ka\",\n",
    "#   \"dbtable\" -> s\"($query) as tbl\",\n",
    "#   \"driver\" -> \"com.teradata.jdbc.TeraDriver\"\n",
    "# )).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exp_smoothing_average_weights(alpha,dim):\n",
    "    a = np.zeros(dim)\n",
    "    for i in range(dim):\n",
    "        a[i] = alpha * ((1-alpha) ** i)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponential smoothing\n",
    "def exp_smoothing_average(alpha,x,axis):\n",
    "    if axis.size == 0:\n",
    "        if len(x) >1:\n",
    "            s_t  = alpha*x[-1] + (1 - alpha)*exp_smoothing_average(alpha,x[0:-1])\n",
    "        else:\n",
    "            s_t = x[0]\n",
    "    elif axis = 0:\n",
    "        s_t = np.zeros(x.shape[0])\n",
    "        for i in range(x.shape[0]):\n",
    "            s_t[i] = exp_smoothing_average(alpha,x[i,:],[])\n",
    " \n",
    "            \n",
    "    return s_t\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
