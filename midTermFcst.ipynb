{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, HiveContext\n",
    "import os\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.storagelevel \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pdb\n",
    "import subprocess # Used for executing linux commands, for writing to teradata\n",
    "import sys\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf, min, max\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from_jupyter = False\n",
    "\n",
    "if \"sc\"  in globals():\n",
    "        SparkContext.stop(sc)\n",
    "        from_jupyter = True\n",
    "                \n",
    "SparkContext.setSystemProperty('spark.yarn.queue','gg-uk-tescoglobal-hadoop-rdf')\n",
    "SparkContext.setSystemProperty('spark.executor.instances', '50')\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '5g')\n",
    "#SparkContext.setSystemProperty('spark.yarn.executor.memoryOverhead','10000')\n",
    "SparkContext.setSystemProperty('spark.executor.cores', '1')\n",
    "SparkContext.setSystemProperty('spark.sql.shuffle.partitions', '10000')\n",
    "SparkContext.setSystemProperty('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')\n",
    "SparkContext.setSystemProperty('spark.default.parallelism','2000')\n",
    "#SparkContext.setSystemProperty('spark.driver.maxResultSize','20g')\n",
    "SparkContext.setSystemProperty('spark.executor.heartbeatInterval','60s')\n",
    "\n",
    "if from_jupyter:\n",
    "    sc = SparkContext(\"yarn-client\", \"Mid term Forecast\")\n",
    "else:\n",
    "    conf = SparkConf().setAppName(\"Mid term Forecast\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-16 05:56:52.128644\n"
     ]
    }
   ],
   "source": [
    "print datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-15 2017-10-16 2018-01-15 20171016 20180115\n"
     ]
    }
   ],
   "source": [
    "# Parameters that are required as prerequisites\n",
    "forecast_start_day = 0\n",
    "forecast_end_day = 91\n",
    "\n",
    "# current_date = '2017-08-16'\n",
    "# forecast_start = '2017-08-17'\n",
    "# forecast_end = '2017-11-15'\n",
    "# cal_fcst_start = '20170817'\n",
    "# cal_fcst_end = '20171115'\n",
    "# sub_group = 'allSG'\n",
    "\n",
    "current_date = (datetime.now()+ timedelta(days=-1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "forecast_start=(datetime.now() + timedelta(days=forecast_start_day)).strftime(\"%Y-%m-%d\")\n",
    "forecast_end=(datetime.now() + timedelta(days=forecast_end_day)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "forecast_start_date=(datetime.now() + timedelta(days=forecast_start_day)).date()\n",
    "forecast_end_date=(datetime.now() + timedelta(days=forecast_end_day)).date()\n",
    "\n",
    "cal_fcst_start = (datetime.now() + timedelta(days=forecast_start_day)).strftime(\"%Y%m%d\")\n",
    "cal_fcst_end = (datetime.now() + timedelta(days=forecast_end_day)).strftime(\"%Y%m%d\")\n",
    "sub_group='F11MA'\n",
    "\n",
    "print current_date, forecast_start, forecast_end, cal_fcst_start, cal_fcst_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Lets get the CAL for the date range that we are interested in.\n",
    "query = \"(select * from DXWI_PROD_VIEW_ACCESS.VWI0CAL_CALENDAR \\\n",
    "          where calendar_date between '{0}' and '{1}') as CAL_DATA order by CAL_DATA.Calendar_Date\" \\\n",
    "         .format(cal_fcst_start, cal_fcst_end)\n",
    "\n",
    "cal_data = (sqlContext.read.format('jdbc')\n",
    "                .options(url=\"jdbc:teradata://TDPS.ukroi.tesco.org/\",\n",
    "                     dbtable=query,\n",
    "                     user = \"xg66\",\n",
    "                     password = \"tesco123\",\n",
    "                     driver = \"com.teradata.jdbc.TeraDriver\")\n",
    "                .option(\"treatEmptyValuesAsNulls\",\"true\")\n",
    "                .option(\"inferSchema\",\"false\")\n",
    "               # .option(\"nullValues\",\"null\")\n",
    "                .load()\n",
    "                .cache()\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------+--------------+-------------+-----------+----------+---------+------------------+--------+--------+-------------------+-------------------+---------------------+\n",
      "|Calendar_Date|Year_Week_Number|Year_Number|Quarter_Number|Period_Number|Week_Number|Day_Number| Day_Text|Period_Week_Number|Tesco_WF|Tesco_PF|Commercial_Tesco_WF|Commercial_Tesco_PF|Commercial_Day_Number|\n",
      "+-------------+----------------+-----------+--------------+-------------+-----------+----------+---------+------------------+--------+--------+-------------------+-------------------+---------------------+\n",
      "|   2017-10-16|          201734|       2017|             3|            8|         34|         1|MONDAY   |                 3| 2017341|20170831|            2017342|           20170832|                    2|\n",
      "|   2017-10-17|          201734|       2017|             3|            8|         34|         2|TUESDAY  |                 3| 2017342|20170832|            2017343|           20170833|                    3|\n",
      "+-------------+----------------+-----------+--------------+-------------+-----------+----------+---------+------------------+--------+--------+-------------------+-------------------+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cal_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cal_sel_data = cal_data.select(col(\"CALENDAR_DATE\").alias(\"calendar_date\").cast(DateType()),\n",
    "                               col(\"YEAR_WEEK_NUMBER\").alias(\"year_week\").cast(StringType()),\n",
    "                               col(\"WEEK_NUMBER\").alias(\"week_number\").cast(StringType()),\n",
    "                               col(\"DAY_NUMBER\").alias(\"day_number\").cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_sel_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cal_fcst_start_year_week  =  cal_sel_data.select(min(\"year_week\"),max(\"year_week\")).collect()[0][\"min(year_week)\"]\n",
    "cal_fcst_end_year_week  = cal_sel_data.select(min(\"year_week\"),max(\"year_week\")).collect()[0][\"max(year_week)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'201734'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_sel_data.select(min(\"year_week\"),max(\"year_week\")).collect()[0][\"min(year_week)\"]\n",
    "#cal_sel_data.select(min(\"year_week\"),max(\"year_week\")).collect()[0][\"max(year_week)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cal_list = cal_sel_data.rdd.map(lambda x : (x.calendar_date,(x.year_week,x.week_number,x.day_number))).collectAsMap()\n",
    "# cal_bc_var = sc.broadcast(cal_list)\n",
    "# current_date_bc_var = sc.broadcast(current_date)\n",
    "##vj_remove cal_sel_data.write.orc('hdfs://pphdp/insight_labs/rdf/output/cr_forecast/cal_bc_set1_'+sub_group,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input for F11MA sub group\n",
    "sub_group='F11MA'\n",
    "nws_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/source_image/cr_forecast/nws')\n",
    "rrt_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/source_image/cr_forecast/rrt')\n",
    "rst_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/source_image/cr_forecast/rst')\n",
    "dst_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/source_image/cr_forecast/dst')\n",
    "dtp_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/source_image/cr_forecast/ptp')\n",
    "ptg_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/source_image/cr_forecast/ptg')\n",
    "swf_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/source_image/cr_forecast/swf')\n",
    "wef_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/source_image/cr_forecast/wef')\n",
    "\n",
    "# Input for all sub groups\n",
    "#sub_group = 'allSG'\n",
    "#nws_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/source_image/golden_input/nws')\n",
    "#rrt_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/source_image/golden_input/rrt')\n",
    "#rst_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/source_image/golden_input/rst')\n",
    "#dst_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/source_image/golden_input/dst')\n",
    "#dtp_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/source_image/golden_input/ptp')\n",
    "#ptg_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/source_image/golden_input/ptg')\n",
    "#swf_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/source_image/golden_input/swf')\n",
    "#wef_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/source_image/golden_input/wef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nws_sel_data = nws_data.select(col(\"RETAIL_OUTLET_NUMBER\").alias(\"ro_no\").cast(IntegerType()),\n",
    "                               col(\"BASE_PRODUCT_NUMBER\").alias(\"bpr_tpn\").cast(IntegerType()),\n",
    "                               col(\"SUB_GROUP_CODE\").alias(\"sg_cd\").cast(StringType()),\n",
    "                               col(\"NWS_STAT\").alias(\"nws_stat\").cast(StringType()),\n",
    "                               col(\"STEP_IND\").alias(\"step_ind\").cast(StringType()),\n",
    "                               col(\"FINAL_NWS\").alias(\"final_nws\").cast(DoubleType()),\n",
    "                               col(\"SAVED_FINAL_NWS\").alias(\"saved_final_nws\").cast(DoubleType()),\n",
    "                               col(\"AGGR_GRP_ID\").alias(\"aggr_grp_id\").cast(StringType()),\n",
    "                               col(\"STOCKED_PROD_START_DATE\").alias(\"stkd_prod_stdt\").cast(DateType()),\n",
    "                               col(\"STOCKED_PROD_END_DATE\").alias(\"stkd_prod_endt\").cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nws_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nws_merged_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if nws_sel_data.rdd.getNumPartitions() < 2000:\n",
    "            nws_sel_data  = nws_sel_data.repartition(2000).cache()\n",
    "\n",
    "nws_schema = StructType([StructField(\"nws_key\",StructType([StructField(\"ro_no\",IntegerType(),True),\n",
    "                                                           StructField(\"bpr_tpn\",IntegerType(),True)]),True),\n",
    "                         StructField(\"nws_merged_values\",\n",
    "                                              ArrayType(StructType([StructField(\"sg_cd\",StringType(),True),\n",
    "                                                                    StructField(\"nws_stat\",StringType(),True),\n",
    "                                                                    StructField(\"step_ind\",StringType(),True),\n",
    "                                                                    StructField(\"final_nws\",DoubleType(),True),\n",
    "                                                                    StructField(\"saved_final_nws\",DoubleType(),True),\n",
    "                                                                    StructField(\"aggr_grp_id\",StringType(),True),\n",
    "                                                                    StructField(\"stkd_prod_stdt\",DateType(),True),\n",
    "                                                                    StructField(\"stkd_prod_endt\",DateType(),True)]),True),\n",
    "                                     True)\n",
    "                        ]\n",
    "                       )\n",
    "nws_merged_rdd = (nws_sel_data\n",
    "                         .withColumn(\"key\",struct([col(\"ro_no\"),\n",
    "                                                   col(\"bpr_tpn\")\n",
    "                                                  ]))\n",
    "                         .withColumn(\"value\",array([struct([\n",
    "                                                           col(\"sg_cd\"),\n",
    "                                                           col(\"nws_stat\"),\n",
    "                                                           col(\"step_ind\"),\n",
    "                                                           col(\"final_nws\"),\n",
    "                                                           col(\"saved_final_nws\"),\n",
    "                                                           col(\"aggr_grp_id\"),\n",
    "                                                           col(\"stkd_prod_stdt\"),\n",
    "                                                           col(\"stkd_prod_endt\")\n",
    "                                                           ])\n",
    "                                                 ])\n",
    "                                    )\n",
    "                         .select(\"key\",\"value\")\n",
    "                         .rdd\n",
    "                         .reduceByKey(lambda v1,v2 : v1 + v2)\n",
    "                         #.toDF([\"nws_key\",\"nws_merged_values\"])\n",
    "                )\n",
    "\n",
    "nws_merged_df = sqlContext.createDataFrame(nws_merged_rdd,nws_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51666"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nws_merged_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|        nws_key|   nws_merged_values|\n",
      "+---------------+--------------------+\n",
      "|[2920,81003974]|[[F11BD,W,Y,381.9...|\n",
      "|[5700,54550994]|[[F51CA,A,N,968.1...|\n",
      "|[2586,52466256]|[[F51CA,A,N,279.9...|\n",
      "|[3190,50962236]|[[F11MA,W,Y,86.06...|\n",
      "|[6807,54550971]|[[F51CA,A,N,84.87...|\n",
      "+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nws_merged_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rrt_sel_data = rrt_data.select(col(\"RETAIL_OUTLET_NUMBER\").alias(\"ro_no\").cast(IntegerType()),\n",
    "                               col(\"BASE_PRODUCT_NUMBER\").alias(\"bpr_tpn\").cast(IntegerType()),\n",
    "                               col(\"STEP_EFF_DT\").alias(\"rrt_step_eff_dt\").cast(DateType()),\n",
    "                               col(\"STEP_TYPE_NUM\").alias(\"rrt_step_type_num\").cast(IntegerType()),\n",
    "                               col(\"STEP_UPLIFT_PC\").alias(\"rrt_step_uplift\").cast(IntegerType()),\n",
    "                               col(\"STEP_VOLUME\").alias(\"rrt_step_volume\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|        rrt_key|   rrt_merged_values|\n",
      "+---------------+--------------------+\n",
      "|[2468,54739758]|[[2017-10-17,251,...|\n",
      "|[2364,57755937]|[[2017-10-15,251,...|\n",
      "|[2927,50502269]|[[2017-10-15,351,...|\n",
      "+---------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if rrt_sel_data.rdd.isEmpty():\n",
    "    schema=StructType([StructField(\"rrt_ro_no\", IntegerType(), True), StructField(\"rrt_bpr_tpn\", IntegerType(), True), StructField(\"mergedInput\", StringType(), True)])\n",
    "    rrt_merged_df = sqlContext.createDataFrame(sc.parallelize([(0,0,'0')]),schema=schema)\n",
    "    rrt_merged_rdd = rrt_merged_df.rdd.map(lambda row : ((row.rrt_ro_no,row.rrt_bpr_tpn),row.mergedInput))\n",
    "    rrt_merged_df = rrt_merged_rdd.toDF([\"rrt_key\",\"rrt_merged_values\"])\n",
    "else:\n",
    "    \n",
    "    if rrt_sel_data.rdd.getNumPartitions() < 400:\n",
    "            rrt_sel_data  = rrt_sel_data.repartition(400)\n",
    "   \n",
    "    \n",
    "    rrt_schema = StructType([StructField(\"rrt_key\",StructType([StructField(\"ro_no\",IntegerType(),True),\n",
    "                                                               StructField(\"bpr_tpn\",IntegerType(),True)]),True),\n",
    "                             StructField(\"rrt_merged_values\",\n",
    "                                         ArrayType(StructType([StructField(\"rrt_step_eff_dt\",DateType(),True),\n",
    "                                                               StructField(\"rrt_step_type_num\",IntegerType(),True),\n",
    "                                                               StructField(\"rrt_step_uplift\",IntegerType(),True),\n",
    "                                                               StructField(\"rrt_step_volume\",DoubleType(),True)]),True),\n",
    "                                     True)\n",
    "                        ]\n",
    "                       )\n",
    "    \n",
    "    rrt_merged_rdd = (rrt_sel_data\n",
    "                         .withColumn(\"key\",struct([col(\"ro_no\"),\n",
    "                                                   col(\"bpr_tpn\")]))\n",
    "                         .withColumn(\"value\",array([struct([\n",
    "                                                           col(\"rrt_step_eff_dt\"),\n",
    "                                                           col(\"rrt_step_type_num\"),\n",
    "                                                           col(\"rrt_step_uplift\"),\n",
    "                                                           col(\"rrt_step_volume\")\n",
    "                                                           ])\n",
    "                                                 ])\n",
    "                                    )\n",
    "                         .select(\"key\",\"value\")\n",
    "                         .rdd\n",
    "                         .reduceByKey(lambda v1,v2 : v1 + v2)\n",
    "                         #.toDF([\"rrt_key\",\"rrt_merged_values\"])\n",
    "                        )\n",
    "    rrt_merged_df = sqlContext.createDataFrame(rrt_merged_rdd,rrt_schema)\n",
    "    \n",
    "rrt_merged_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrt_merged_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "join_cond1 = [nws_merged_df.nws_key == rrt_merged_df.rrt_key]\n",
    "nws_rrt_df = nws_merged_df.join(rrt_merged_df,join_cond1,\"left_outer\").drop(\"rrt_key\")#.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rst_data = rst_data.repartition(1000)\n",
    "rst_sel_data = rst_data.select(col(\"RETAIL_OUTLET_NUMBER\").alias(\"ro_no\").cast(IntegerType()),\n",
    "                               col(\"BASE_PRODUCT_NUMBER\").alias(\"bpr_tpn\").cast(IntegerType()),\n",
    "                               col(\"STEP_EFF_DT\").alias(\"rst_step_eff_dt\").cast(DateType()),\n",
    "                               col(\"STEP_END_DT\").alias(\"rst_step_end_dt\").cast(DateType()),\n",
    "                               col(\"STEP_TYPE_NUM\").alias(\"rst_step_type_num\").cast(IntegerType()),\n",
    "                               col(\"STEP_ID\").alias(\"rst_step_id\").cast(IntegerType()),\n",
    "                               col(\"UNDERLYING_STEP_ID\").alias(\"rst_underlying_step_id\").cast(IntegerType()),\n",
    "                               col(\"STEP_UPLIFT_PC\").alias(\"rst_step_uplift\").cast(IntegerType()),\n",
    "                               col(\"STEP_VOLUME\").alias(\"rst_step_volume\").cast(DoubleType()),\n",
    "                               col(\"ACTUAL_NWS\").alias(\"rst_actual_nws\").cast(DoubleType()),\n",
    "                               col(\"WTHR_SENS_FLAG\").alias(\"rst_wthr_sens_flag\").cast(StringType()),\n",
    "                               col(\"CONTINUE_NWS\").alias(\"rst_continue_nws\").cast(StringType())\n",
    "                              )\n",
    "#rst_sel_data.filter(rst_sel_data.ro_no==5288).filter(rst_sel_data.bpr_tpn==50935010).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---------------+---------------+-----------------+-----------+----------------------+---------------+---------------+--------------+------------------+----------------+\n",
      "|ro_no| bpr_tpn|rst_step_eff_dt|rst_step_end_dt|rst_step_type_num|rst_step_id|rst_underlying_step_id|rst_step_uplift|rst_step_volume|rst_actual_nws|rst_wthr_sens_flag|rst_continue_nws|\n",
      "+-----+--------+---------------+---------------+-----------------+-----------+----------------------+---------------+---------------+--------------+------------------+----------------+\n",
      "| 6158|54550994|     2017-10-13|     2017-10-23|              241|  113100467|                     0|              0|           null|        425.52|                 Y|               N|\n",
      "| 6545|61205253|     2017-10-16|     2017-10-16|              241|  114212202|                     0|            -10|           null|          null|                 Y|               N|\n",
      "+-----+--------+---------------+---------------+-----------------+-----------+----------------------+---------------+---------------+--------------+------------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rst_sel_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20170809 20190915\n"
     ]
    }
   ],
   "source": [
    "# We need the minimum of step start and maximum of step end so that the same can be used to get the CAL range\n",
    "rst_fcst_start = str(rst_sel_data.select(min(\"rst_step_eff_dt\")).flatMap(lambda x:x).collect()[0]).replace('-','')\n",
    "rst_fcst_end = str(rst_sel_data.select(max(\"rst_step_end_dt\")).flatMap(lambda x:x).collect()[0]).replace('-','')\n",
    "\n",
    "if rst_fcst_start > cal_fcst_start:\n",
    "    rst_fcst_start = cal_fcst_start\n",
    "    \n",
    "if rst_fcst_end < cal_fcst_end:\n",
    "    rst_fcst_end = cal_fcst_end\n",
    "    \n",
    "print rst_fcst_start, rst_fcst_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's get the forecast range interms of year week from CAL for the range of dates as obtained above. \n",
    "# This is needed to get only required PTP/PTG/SWF\n",
    "query = \"(select * from DXWI_PROD_VIEW_ACCESS.VWI0CAL_CALENDAR \\\n",
    "          where calendar_date between '{0}' and '{1}') as CAL_DATA order by CAL_DATA.Calendar_Date\" \\\n",
    "         .format(rst_fcst_start, rst_fcst_end)\n",
    "\n",
    "cal_data = (sqlContext.read.format('jdbc')\n",
    "                .options(url=\"jdbc:teradata://TDPS.ukroi.tesco.org/\",\n",
    "                     dbtable=query,\n",
    "                     user = \"xg66\",\n",
    "                     password = \"tesco123\",\n",
    "                     driver = \"com.teradata.jdbc.TeraDriver\")\n",
    "                .option(\"treatEmptyValuesAsNulls\",\"true\")\n",
    "                .option(\"inferSchema\",\"false\")\n",
    "               # .option(\"nullValues\",\"null\")\n",
    "                .load()\n",
    "                .cache()\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cal_sel_data = cal_data.select(col(\"CALENDAR_DATE\").alias(\"calendar_date\").cast(DateType()),\n",
    "                               col(\"YEAR_WEEK_NUMBER\").alias(\"year_week\").cast(StringType()),\n",
    "                               col(\"WEEK_NUMBER\").alias(\"week_number\").cast(StringType()),\n",
    "                               col(\"DAY_NUMBER\").alias(\"day_number\").cast(StringType()))\n",
    "#cal_sel_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# cal_list = cal_sel_data.rdd.map(lambda x : (x.calendar_date,(x.year_week,x.week_number,x.day_number))).collectAsMap()\n",
    "# cal_bc_var2 = sc.broadcast(cal_list)\n",
    "#vijay cal_sel_data.write.orc('hdfs://pphdp/insight_labs/rdf/output/cr_forecast/cal_bc_set2_'+sub_group,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cal_yrwk_start = str(cal_data.select(min(\"Year_Week_Number\")).flatMap(lambda x:x).collect()[0])\n",
    "cal_yrwk_end = str(cal_data.select(max(\"Year_Week_Number\")).flatMap(lambda x:x).collect()[0])\n",
    "\n",
    "#print cal_yrwk_start, cal_yrwk_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------+----------+\n",
      "|ro_no| bpr_tpn|       max|       min|\n",
      "+-----+--------+----------+----------+\n",
      "| 2894|50502269|2017-11-07|2017-10-18|\n",
      "| 2848|81003974|2017-10-16|2017-10-16|\n",
      "| 2388|54550994|2017-10-30|2017-10-13|\n",
      "| 6849|57755937|2017-10-24|2017-10-13|\n",
      "+-----+--------+----------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify the minimum step eff date and maximum step end date for every store product\n",
    "rst_sel_data_groupBy = (rst_sel_data.groupBy(\"ro_no\",\"bpr_tpn\")\n",
    "                        .agg({\"rst_step_eff_dt\":\"min\",\"rst_step_end_dt\":\"max\"})\n",
    "                        .withColumnRenamed(\"min(rst_step_eff_dt)\",\"min\")\n",
    "                        .withColumnRenamed(\"max(rst_step_end_dt)\",\"max\"))\n",
    "rst_sel_data_groupBy.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compare the max and min dates with the forecast range using UDF\n",
    "min_start_date = udf(lambda x : x if x < forecast_start_date else forecast_start_date, DateType())\n",
    "max_end_date = udf(lambda x : x if x > forecast_end_date else forecast_end_date, DateType())\n",
    "\n",
    "rst_sel_data_groupBy_2 = rst_sel_data_groupBy.select(\"*\",\n",
    "                                                     min_start_date(col(\"min\")).alias(\"min_fcst_start_date\"),\n",
    "                                                     max_end_date(col(\"max\")).alias(\"max_fcst_end_date\"))\n",
    "\n",
    "#rst_sel_data_groupBy_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------------+-----------------+------------------------+----------------------+\n",
      "|ro_no| bpr_tpn|min_fcst_start_date|max_fcst_end_date|min_fcst_start_year_week|max_fcst_end_year_week|\n",
      "+-----+--------+-------------------+-----------------+------------------------+----------------------+\n",
      "| 2894|50502269|         2017-10-16|       2018-01-15|                  201734|                  null|\n",
      "| 2388|54550994|         2017-10-13|       2018-01-15|                  201733|                  null|\n",
      "| 6849|57755937|         2017-10-13|       2018-01-15|                  201733|                  null|\n",
      "+-----+--------+-------------------+-----------------+------------------------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the above with CAL table to arrive at the year week format for min_fcst_start_date and max_fcst_end_date\n",
    "rst_sel_data_groupBy_3 = (rst_sel_data_groupBy_2.join(cal_sel_data,\n",
    "                                                    rst_sel_data_groupBy_2[\"min_fcst_start_date\"] == cal_sel_data.calendar_date,\n",
    "                                                     \"left_outer\")\n",
    "                                                    .select(\"ro_no\"\n",
    "                                                           ,\"bpr_tpn\"\n",
    "                                                           ,\"min_fcst_start_date\"\n",
    "                                                           ,\"max_fcst_end_date\"\n",
    "                                                           ,\"year_week\")\n",
    "                                                    .withColumnRenamed(\"year_week\",\"min_fcst_start_year_week\")\n",
    "                                                .join(cal_sel_data,\n",
    "                                                     rst_sel_data_groupBy_2[\"max_fcst_end_date\"] == cal_sel_data.calendar_date,\n",
    "                                                     \"left_outer\")\n",
    "                                                    .select(\"ro_no\"\n",
    "                                                           ,\"bpr_tpn\"\n",
    "                                                           ,\"min_fcst_start_date\"\n",
    "                                                           ,\"max_fcst_end_date\"\n",
    "                                                           ,\"min_fcst_start_year_week\"\n",
    "                                                           ,\"year_week\")\n",
    "                                                    .withColumnRenamed(\"year_week\",\"max_fcst_end_year_week\"))\n",
    "\n",
    "rst_sel_data_groupBy_3.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|        rst_key|   rst_merged_values|\n",
      "+---------------+--------------------+\n",
      "|[2920,81003974]|[[2017-10-16,2017...|\n",
      "|[5992,50965158]|[[2017-10-16,2017...|\n",
      "+---------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if rst_sel_data.rdd.isEmpty():\n",
    "    schema=StructType([StructField(\"ro_no\", IntegerType(), True), StructField(\"bpr_tpn\", IntegerType(), True), StructField(\"mergedInput\", StringType(), True)])\n",
    "    rst_merged_df = sqlContext.createDataFrame(sc.parallelize([(0,0,'0')]),schema=schema)\n",
    "    rst_merged_rdd = rst_merged_df.rdd.map(lambda row : ((row.rst_ro_no,row.rst_bpr_tpn),row.mergedInput))\n",
    "    rst_merged_df = rst_merged_rdd.toDF([\"rst_key\",\"rst_merged_values\"])\n",
    "else:\n",
    "    \n",
    "    if rst_sel_data.rdd.getNumPartitions() < 400:\n",
    "            rst_sel_data  = rst_sel_data.repartition(400)\n",
    "   \n",
    "    rst_schema = StructType([StructField(\"rst_key\",StructType([StructField(\"ro_no\",IntegerType(),True),\n",
    "                                                               StructField(\"bpr_tpn\",IntegerType(),True)]),True),\n",
    "                             StructField(\"rst_merged_values\",\n",
    "                                         ArrayType(StructType([StructField(\"rst_step_eff_dt\",DateType(),True),\n",
    "                                                               StructField(\"rst_step_end_dt\",DateType(),True),\n",
    "                                                               StructField(\"rst_step_type_num\",IntegerType(),True),\n",
    "                                                               StructField(\"rst_step_id\",IntegerType(),True),\n",
    "                                                               StructField(\"rst_underlying_step_id\",IntegerType(),True),\n",
    "                                                               StructField(\"rst_step_uplift\",IntegerType(),True),\n",
    "                                                               StructField(\"rst_step_volume\",DoubleType(),True),\n",
    "                                                               StructField(\"rst_actual_nws\",DoubleType(),True),\n",
    "                                                               StructField(\"rst_wthr_sens_flag\",StringType(),True),\n",
    "                                                               StructField(\"rst_continue_nws\",StringType(),True)]),True),\n",
    "                                     True)\n",
    "                        ]\n",
    "                       )\n",
    "    rst_merged_rdd = (rst_sel_data\n",
    "                         .withColumn(\"key\",struct([col(\"ro_no\"),col(\"bpr_tpn\")]))\n",
    "                         .withColumn(\"value\",array([struct([\n",
    "                                                           col(\"rst_step_eff_dt\"),\n",
    "                                                           col(\"rst_step_end_dt\"),\n",
    "                                                           col(\"rst_step_type_num\"),\n",
    "                                                           col(\"rst_step_id\"),\n",
    "                                                           col(\"rst_underlying_step_id\"),\n",
    "                                                           col(\"rst_step_uplift\"),\n",
    "                                                           col(\"rst_step_volume\"),\n",
    "                                                           col(\"rst_actual_nws\"),\n",
    "                                                           col(\"rst_wthr_sens_flag\"),\n",
    "                                                           col(\"rst_continue_nws\")\n",
    "                                                           ]\n",
    "                                                          )\n",
    "                                                 ])\n",
    "                                    )\n",
    "                         .select(\"key\",\"value\")\n",
    "                         .rdd\n",
    "                         .reduceByKey(lambda v1,v2 : v1 + v2)\n",
    "                         #.toDF([\"rst_key\",\"rst_merged_values\"])\n",
    "                        )\n",
    "\n",
    "    rst_merged_df = sqlContext.createDataFrame(rst_merged_rdd,rst_schema)\n",
    "    \n",
    "rst_merged_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-----------------+--------------------+\n",
      "|        nws_key|   nws_merged_values|rrt_merged_values|   rst_merged_values|\n",
      "+---------------+--------------------+-----------------+--------------------+\n",
      "|[2139,50502413]|[[F13DA,W,Y,277.0...|             null|                null|\n",
      "|[2518,54550994]|[[F51CA,A,N,588.8...|             null|                null|\n",
      "|[2564,50965158]|[[F11MA,W,Y,18.77...|             null|[[2017-10-16,2017...|\n",
      "+---------------+--------------------+-----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_cond2 = [nws_rrt_df.nws_key == rst_merged_df.rst_key]\n",
    "\n",
    "nws_rrt_rst_df = nws_rrt_df.join(rst_merged_df,join_cond2,\"leftouter\").drop(\"rst_key\")#.cache()\n",
    "\n",
    "nws_rrt_rst_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------+------------+-----------+----------------------+---------------+\n",
      "|ro_no| bpr_tpn|dst_start_date|dst_end_date|dst_step_id|dst_underlying_step_id|dst_stack_index|\n",
      "+-----+--------+--------------+------------+-----------+----------------------+---------------+\n",
      "| 2098|84128142|    2017-10-16|  2017-10-31|  129238922|                     0|              1|\n",
      "| 6331|82639698|    2017-10-11|  2017-11-21|  119999696|                     0|              1|\n",
      "| 2061|57092822|    2017-10-25|  2017-11-14|  115615265|                     0|              1|\n",
      "+-----+--------+--------------+------------+-----------+----------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dst_sel_data = dst_data.select(col(\"RETAIL_OUTLET_NUMBER\").alias(\"ro_no\").cast(IntegerType()),\n",
    "                               col(\"BASE_PRODUCT_NUMBER\").alias(\"bpr_tpn\").cast(IntegerType()),\n",
    "                               col(\"START_DATE\").alias(\"dst_start_date\").cast(DateType()),\n",
    "                               col(\"END_DATE\").alias(\"dst_end_date\").cast(DateType()),\n",
    "                               col(\"STEP_ID\").alias(\"dst_step_id\").cast(IntegerType()),\n",
    "                               col(\"UNDERLYING_STEP_ID\").alias(\"dst_underlying_step_id\").cast(IntegerType()),\n",
    "                               col(\"STACK_INDEX\").alias(\"dst_stack_index\").cast(IntegerType())\n",
    "                              )\n",
    "\n",
    "dst_sel_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|        dst_key|   dst_merged_values|\n",
      "+---------------+--------------------+\n",
      "|[5570,64438600]|[[2017-10-23,2017...|\n",
      "|[3236,51820386]|[[2017-11-15,2017...|\n",
      "|[2202,81576272]|[[2017-10-04,2017...|\n",
      "+---------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if dst_sel_data.rdd.isEmpty():\n",
    "    schema=StructType([StructField(\"ro_no\", IntegerType(), True), StructField(\"bpr_tpn\", IntegerType(), True), StructField(\"mergedInput\", StringType(), True)])\n",
    "    dst_merged_df = sqlContext.createDataFrame(sc.parallelize([(0,0,'0')]),schema=schema)\n",
    "    dst_merged_rdd = dst_merged_df.rdd.map(lambda row : ((row.dst_ro_no,row.dst_bpr_tpn),row.mergedInput))\n",
    "    dst_merged_df = dst_merged_rdd.toDF([\"dst_key\",\"dst_merged_values\"])\n",
    "else:\n",
    "    \n",
    "    if dst_sel_data.rdd.getNumPartitions() < 400:\n",
    "            dst_sel_data  = dst_sel_data.repartition(400)\n",
    "    \n",
    "    dst_schema = StructType([StructField(\"dst_key\",StructType([StructField(\"ro_no\",IntegerType(),True),\n",
    "                                                               StructField(\"bpr_tpn\",IntegerType(),True)]),True),\n",
    "                             StructField(\"dst_merged_values\",\n",
    "                                         ArrayType(StructType([StructField(\"dst_start_date\",DateType(),True),\n",
    "                                                               StructField(\"dst_end_date\",DateType(),True),\n",
    "                                                               StructField(\"dst_step_id\",IntegerType(),True),\n",
    "                                                               StructField(\"dst_underlying_step_id\",IntegerType(),True),\n",
    "                                                               StructField(\"dst_stack_index\",IntegerType(),True)]),True),\n",
    "                                     True)\n",
    "                        ]\n",
    "                       )\n",
    "    dst_merged_rdd = (dst_sel_data\n",
    "                         .withColumn(\"key\",struct([col(\"ro_no\"),\n",
    "                                                   col(\"bpr_tpn\")]))\n",
    "                         .withColumn(\"value\",array([struct([col(\"dst_start_date\"),\n",
    "                                                             col(\"dst_end_date\"),\n",
    "                                                             col(\"dst_step_id\"),\n",
    "                                                             col(\"dst_underlying_step_id\"),\n",
    "                                                             col(\"dst_stack_index\")]\n",
    "                                                          )\n",
    "                                                 ])\n",
    "                                    )\n",
    "                         .select(\"key\",\"value\")\n",
    "                         .rdd\n",
    "                         .reduceByKey(lambda v1,v2 : v1 + v2)\n",
    "                         #.toDF([\"dst_key\",\"dst_merged_values\"])\n",
    "                        )\n",
    "    dst_merged_df = sqlContext.createDataFrame(dst_merged_rdd,dst_schema)\n",
    "dst_merged_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-----------------+--------------------+--------------------+\n",
      "|        nws_key|   nws_merged_values|rrt_merged_values|   rst_merged_values|   dst_merged_values|\n",
      "+---------------+--------------------+-----------------+--------------------+--------------------+\n",
      "|[2139,50502413]|[[F13DA,W,Y,277.0...|             null|                null|                null|\n",
      "|[2518,54550994]|[[F51CA,A,N,588.8...|             null|                null|                null|\n",
      "|[2564,50965158]|[[F11MA,W,Y,18.77...|             null|[[2017-10-16,2017...|[[2017-10-16,2017...|\n",
      "+---------------+--------------------+-----------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_cond2 = [nws_rrt_rst_df.nws_key == dst_merged_df.dst_key]\n",
    "\n",
    "nws_rrt_rst_dst_df = nws_rrt_rst_df.join(dst_merged_df,join_cond2,\"leftouter\").drop(\"dst_key\")#.cache()\n",
    "\n",
    "nws_rrt_rst_dst_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------+-----+-----+-----+-----+-----+-----+-----+\n",
      "|ro_no| bpr_tpn|dtp_year_week|ptp_1|ptp_2|ptp_3|ptp_4|ptp_5|ptp_6|ptp_7|\n",
      "+-----+--------+-------------+-----+-----+-----+-----+-----+-----+-----+\n",
      "| 2132|50934973|       201733| 15.6| 12.4| 11.9| 13.4| 17.0| 19.2| 10.5|\n",
      "| 2132|50934973|       201734| 15.6| 12.4| 11.9| 13.4| 17.0| 19.2| 10.5|\n",
      "| 2132|50934973|       201741| 15.6| 12.4| 11.9| 13.4| 17.0| 19.2| 10.5|\n",
      "| 2132|50934973|       201745| 15.6| 12.4| 11.9| 13.4| 17.0| 19.2| 10.5|\n",
      "| 2132|50934973|       201742| 15.6| 12.4| 11.9| 13.4| 17.0| 19.2| 10.5|\n",
      "| 2132|50934973|       201737| 15.6| 12.4| 11.9| 13.4| 17.0| 19.2| 10.5|\n",
      "| 2132|50934973|       201739| 15.6| 12.4| 11.9| 13.4| 17.0| 19.2| 10.5|\n",
      "| 2132|50934973|       201743| 15.6| 12.4| 11.9| 13.4| 17.0| 19.2| 10.5|\n",
      "| 2132|50934973|       201738| 15.6| 12.4| 11.9| 13.4| 17.0| 19.2| 10.5|\n",
      "| 2132|50934973|       201736| 15.6| 12.4| 11.9| 13.4| 17.0| 19.2| 10.5|\n",
      "+-----+--------+-------------+-----+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtp_sel_data = dtp_data.filter(dtp_data.year_week_number >= cal_yrwk_start).filter(dtp_data.year_week_number <= cal_yrwk_end)\\\n",
    ".select(col(\"RETAIL_OUTLET_NUMBER\").alias(\"ro_no\").cast(IntegerType()),\n",
    "                               col(\"BASE_PRODUCT_NUMBER\").alias(\"bpr_tpn\").cast(IntegerType()),\n",
    "                               col(\"YEAR_WEEK_NUMBER\").alias(\"dtp_year_week\").cast(StringType()),\n",
    "                               col(\"PTP_1\").alias(\"ptp_1\").cast(DoubleType()),\n",
    "                               col(\"PTP_2\").alias(\"ptp_2\").cast(DoubleType()),\n",
    "                               col(\"PTP_3\").alias(\"ptp_3\").cast(DoubleType()),\n",
    "                               col(\"PTP_4\").alias(\"ptp_4\").cast(DoubleType()),\n",
    "                               col(\"PTP_5\").alias(\"ptp_5\").cast(DoubleType()),\n",
    "                               col(\"PTP_6\").alias(\"ptp_6\").cast(DoubleType()),\n",
    "                               col(\"PTP_7\").alias(\"ptp_7\").cast(DoubleType())\n",
    "                               )#.cache()\n",
    "\n",
    "dtp_sel_data.filter(dtp_sel_data.ro_no==2132).filter(dtp_sel_data.bpr_tpn==50934973).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------+-----+-----+-----+-----+-----+-----+-----+-------------------+-----------------+------------------------+----------------------+\n",
      "|ro_no| bpr_tpn|dtp_year_week|ptp_1|ptp_2|ptp_3|ptp_4|ptp_5|ptp_6|ptp_7|min_fcst_start_date|max_fcst_end_date|min_fcst_start_year_week|max_fcst_end_year_week|\n",
      "+-----+--------+-------------+-----+-----+-----+-----+-----+-----+-----+-------------------+-----------------+------------------------+----------------------+\n",
      "| 2894|50502269|       201742| 13.9| 11.6| 12.1| 15.0| 18.4| 17.4| 11.6|         2017-10-16|       2018-01-15|                  201734|                201747|\n",
      "| 2894|50502269|       201748| 13.9| 11.6| 12.1| 15.0| 18.4| 17.4| 11.6|         2017-10-16|       2018-01-15|                  201734|                201747|\n",
      "| 2894|50502269|       201738| 13.9| 11.6| 12.1| 15.0| 18.4| 17.4| 11.6|         2017-10-16|       2018-01-15|                  201734|                201747|\n",
      "+-----+--------+-------------+-----+-----+-----+-----+-----+-----+-----+-------------------+-----------------+------------------------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join DTP with the RST grouped at store product level to decide the range of dates for which the DTP needs to be picked\n",
    "dtp_join_cond = [dtp_sel_data.ro_no == rst_sel_data_groupBy_3.ro_no\n",
    "                ,dtp_sel_data.bpr_tpn == rst_sel_data_groupBy_3.bpr_tpn]\n",
    "\n",
    "dtp_sel_data_2 = (dtp_sel_data.join(rst_sel_data_groupBy_3\n",
    "                                    ,dtp_join_cond\n",
    "                                    ,\"left_outer\")\n",
    "                                    .drop(rst_sel_data_groupBy_3.ro_no)\n",
    "                                    .drop(rst_sel_data_groupBy_3.bpr_tpn))\n",
    "        \n",
    "dtp_sel_data_2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------+-----+-----+-----+-----+-----+-----+-----+-------------------+-----------------+------------------------+----------------------+\n",
      "|ro_no| bpr_tpn|dtp_year_week|ptp_1|ptp_2|ptp_3|ptp_4|ptp_5|ptp_6|ptp_7|min_fcst_start_date|max_fcst_end_date|min_fcst_start_year_week|max_fcst_end_year_week|\n",
      "+-----+--------+-------------+-----+-----+-----+-----+-----+-----+-----+-------------------+-----------------+------------------------+----------------------+\n",
      "| 2894|50502269|       201745| 13.9| 11.6| 12.1| 15.0| 18.4| 17.4| 11.6|         2017-10-16|       2018-01-15|                  201734|                201747|\n",
      "| 2894|50502269|       201734| 13.9| 11.6| 12.1| 15.0| 18.4| 17.4| 11.6|         2017-10-16|       2018-01-15|                  201734|                201747|\n",
      "| 2894|50502269|       201740| 13.9| 11.6| 12.1| 15.0| 18.4| 17.4| 11.6|         2017-10-16|       2018-01-15|                  201734|                201747|\n",
      "+-----+--------+-------------+-----+-----+-----+-----+-----+-----+-----+-------------------+-----------------+------------------------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace all null values of min_fcst_start_year_week and max_fcst_end_year_week with forecast start and end year week respectively\n",
    "\n",
    "dtp_sel_data_3 = dtp_sel_data_2.na.fill({\"min_fcst_start_year_week\" : cal_fcst_start_year_week,\n",
    "                                         \"max_fcst_end_year_week\" : cal_fcst_end_year_week})\n",
    "\n",
    "dtp_sel_data_3.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtp_sel_data = (dtp_sel_data_3.filter(dtp_sel_data_3.dtp_year_week >= dtp_sel_data_3.min_fcst_start_year_week )\n",
    "                              .filter(dtp_sel_data_3.dtp_year_week <= dtp_sel_data_3.max_fcst_end_year_week))\n",
    "    \n",
    "#dtp_sel_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|        dtp_key|   dtp_merged_values|\n",
      "+---------------+--------------------+\n",
      "|[5099,51252015]|[[201735,14.4,14....|\n",
      "|[6058,52466256]|[[201744,17.0,14....|\n",
      "|[5992,50965158]|[[201737,14.3,12....|\n",
      "+---------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if dtp_sel_data.rdd.isEmpty():\n",
    "    schema=StructType([StructField(\"dtp_ro_no\", IntegerType(), True), StructField(\"dtp_bpr_tpn\", IntegerType(), True), StructField(\"mergedInput\", StringType(), True)])\n",
    "    dtp_merged_df = sqlContext.createDataFrame(sc.parallelize([(0,0,'0')]),schema=schema)\n",
    "    dtp_merged_rdd = dtp_merged_df.rdd.map(lambda row : ((row.dtp_ro_no,row.dtp_bpr_tpn),row.mergedInput))\n",
    "    dtp_merged_df = dtp_merged_rdd.toDF([\"dtp_key\",\"dtp_merged_values\"])\n",
    "else:  \n",
    "    \n",
    "    if dtp_sel_data.rdd.getNumPartitions() < 400:\n",
    "        dtp_sel_data  = dtp_sel_data.repartition(400)\n",
    "  \n",
    "    dtp_schema = StructType([StructField(\"dtp_key\",StructType([StructField(\"ro_no\",IntegerType(),True),\n",
    "                                                               StructField(\"bpr_tpn\",IntegerType(),True)]),True),\n",
    "                             StructField(\"dtp_merged_values\",\n",
    "                                         ArrayType(StructType([StructField(\"dtp_year_week\",StringType(),True),\n",
    "                                                               StructField(\"ptp_1\",DoubleType(),True),\n",
    "                                                               StructField(\"ptp_2\",DoubleType(),True),\n",
    "                                                               StructField(\"ptp_3\",DoubleType(),True),\n",
    "                                                               StructField(\"ptp_4\",DoubleType(),True),\n",
    "                                                               StructField(\"ptp_5\",DoubleType(),True),\n",
    "                                                               StructField(\"ptp_6\",DoubleType(),True),\n",
    "                                                               StructField(\"ptp_7\",DoubleType(),True)]),True),\n",
    "                                     True)\n",
    "                        ]\n",
    "                       )\n",
    "    dtp_merged_rdd = (dtp_sel_data\n",
    "                         .withColumn(\"key\",struct([col(\"ro_no\"),\n",
    "                                                   col(\"bpr_tpn\")]))\n",
    "                         .withColumn(\"value\",array([struct([col(\"dtp_year_week\"),\n",
    "                                                             col(\"ptp_1\"),\n",
    "                                                             col(\"ptp_2\"),\n",
    "                                                             col(\"ptp_3\"),\n",
    "                                                             col(\"ptp_4\"),\n",
    "                                                             col(\"ptp_5\"),\n",
    "                                                             col(\"ptp_6\"),\n",
    "                                                             col(\"ptp_7\")])\n",
    "                                                 ])\n",
    "                                    )\n",
    "                         .select(\"key\",\"value\")\n",
    "                         .rdd\n",
    "                         .reduceByKey(lambda v1,v2 : v1 + v2)\n",
    "                         #.toDF([\"dtp_key\",\"dtp_merged_values\"])\n",
    "                        )\n",
    "    \n",
    "    dtp_merged_df = sqlContext.createDataFrame(dtp_merged_rdd,dtp_schema)\n",
    "\n",
    "dtp_merged_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------+-----+-----+-----+-----+-----+-----+-----+\n",
      "|ro_no| bpr_tpn|ptg_year_week|ptg_1|ptg_2|ptg_3|ptg_4|ptg_5|ptg_6|ptg_7|\n",
      "+-----+--------+-------------+-----+-----+-----+-----+-----+-----+-----+\n",
      "| 5626|78539177|       201736| 18.5| 14.8| 16.7| 14.3| 13.2| 13.8|  9.8|\n",
      "| 2532|65729767|       201736| 13.9| 12.9| 11.8| 14.8| 17.1| 19.5| 10.8|\n",
      "| 3411|52466256|       201735| 14.7| 12.2| 11.4| 14.4| 18.3| 18.0| 12.0|\n",
      "+-----+--------+-------------+-----+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ptg_sel_data = ptg_data.filter(ptg_data.year_week_number >= cal_yrwk_start).filter(ptg_data.year_week_number <= cal_yrwk_end)\\\n",
    ".select(col(\"RETAIL_OUTLET_NUMBER\").alias(\"ro_no\").cast(IntegerType()),\n",
    "                               col(\"BASE_PRODUCT_NUMBER\").alias(\"bpr_tpn\").cast(IntegerType()),\n",
    "                               col(\"YEAR_WEEK_NUMBER\").alias(\"ptg_year_week\").cast(StringType()),\n",
    "                               col(\"PTG_1\").alias(\"ptg_1\").cast(DoubleType()),\n",
    "                               col(\"PTG_2\").alias(\"ptg_2\").cast(DoubleType()),\n",
    "                               col(\"PTG_3\").alias(\"ptg_3\").cast(DoubleType()),\n",
    "                               col(\"PTG_4\").alias(\"ptg_4\").cast(DoubleType()),\n",
    "                               col(\"PTG_5\").alias(\"ptg_5\").cast(DoubleType()),\n",
    "                               col(\"PTG_6\").alias(\"ptg_6\").cast(DoubleType()),\n",
    "                               col(\"PTG_7\").alias(\"ptg_7\").cast(DoubleType())\n",
    "                               )\n",
    "\n",
    "\n",
    "ptg_sel_data =  (ptg_sel_data.filter(col('ptg_1').isNotNull()) \n",
    "                            .filter(col('ptg_2').isNotNull()) \n",
    "                            .filter(col('ptg_3').isNotNull())\n",
    "                            .filter(col('ptg_4').isNotNull()) \n",
    "                            .filter(col('ptg_5').isNotNull()) \n",
    "                            .filter(col('ptg_6').isNotNull()) \n",
    "                            .filter(col('ptg_7').isNotNull())\n",
    "                           # .cache()\n",
    "                )\n",
    "ptg_sel_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------------+-----+-----+-----+-----+-----+-----+-----+-------------------+-----------------+------------------------+----------------------+\n",
      "|ro_no| bpr_tpn|ptg_year_week|ptg_1|ptg_2|ptg_3|ptg_4|ptg_5|ptg_6|ptg_7|min_fcst_start_date|max_fcst_end_date|min_fcst_start_year_week|max_fcst_end_year_week|\n",
      "+-----+--------+-------------+-----+-----+-----+-----+-----+-----+-----+-------------------+-----------------+------------------------+----------------------+\n",
      "| 2894|50502269|       201735| 13.9| 11.6| 12.1| 15.3| 18.8| 17.9| 12.1|         2017-10-16|       2018-01-15|                  201734|                201747|\n",
      "| 2894|50502269|       201736| 14.3| 12.0| 12.1| 15.0| 18.4| 17.4| 11.6|         2017-10-16|       2018-01-15|                  201734|                201747|\n",
      "| 2388|54550994|       201736| 14.2| 13.5| 11.7| 13.9| 17.7| 15.5| 14.1|         2017-10-13|       2018-01-15|                  201733|                201747|\n",
      "+-----+--------+-------------+-----+-----+-----+-----+-----+-----+-----+-------------------+-----------------+------------------------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join PTG with the RST grouped at store product level to decide the range of dates for which the PTG needs to be picked\n",
    "ptg_join_cond = [ptg_sel_data.ro_no == rst_sel_data_groupBy_3.ro_no\n",
    "                ,ptg_sel_data.bpr_tpn == rst_sel_data_groupBy_3.bpr_tpn]\n",
    "\n",
    "ptg_sel_data_2 = (ptg_sel_data.join(rst_sel_data_groupBy_3\n",
    "                                    ,ptg_join_cond\n",
    "                                    ,\"left_outer\")\n",
    "                                    .drop(rst_sel_data_groupBy_3.ro_no)\n",
    "                                    .drop(rst_sel_data_groupBy_3.bpr_tpn))\n",
    "\n",
    "ptg_sel_data_2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace all null values of min_fcst_start_year_week and max_fcst_end_year_week with forecast start and end year week respectively\n",
    "\n",
    "ptg_sel_data_3 = ptg_sel_data_2.na.fill({\"min_fcst_start_year_week\" : cal_fcst_start_year_week,\n",
    "                                         \"max_fcst_end_year_week\" : cal_fcst_end_year_week})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptg_sel_data = (ptg_sel_data_3.filter(ptg_sel_data_3.ptg_year_week >= ptg_sel_data_3.min_fcst_start_year_week )\n",
    "                              .filter(ptg_sel_data_3.ptg_year_week <= ptg_sel_data_3.max_fcst_end_year_week))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|        ptg_key|   ptg_merged_values|\n",
      "+---------------+--------------------+\n",
      "|[2401,68086237]|[[201736,15.2,13....|\n",
      "|[2920,81003974]|[[201736,16.5,14....|\n",
      "|[5992,50965158]|[[201735,14.3,12....|\n",
      "+---------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if ptg_sel_data.rdd.isEmpty():\n",
    "    schema=StructType([StructField(\"ro_no\", IntegerType(), True), StructField(\"bpr_tpn\", IntegerType(), True), StructField(\"mergedInput\", StringType(), True)])\n",
    "    ptg_merged_df = sqlContext.createDataFrame(sc.parallelize([(0,0,'0')]),schema=schema)\n",
    "    ptg_merged_rdd = ptg_merged_df.rdd.map(lambda row : ((row.ptg_ro_no,row.ptg_bpr_tpn),row.mergedInput))\n",
    "    ptg_merged_df = ptg_merged_rdd.toDF([\"ptg_key\",\"ptg_merged_values\"])\n",
    "else:\n",
    "    \n",
    "    if ptg_sel_data.rdd.getNumPartitions() < 400:\n",
    "        ptg_sel_data  = ptg_sel_data.repartition(400)\n",
    "    \n",
    "    ptg_schema = StructType([StructField(\"ptg_key\",StructType([StructField(\"ro_no\",IntegerType(),True),\n",
    "                                                               StructField(\"bpr_tpn\",IntegerType(),True)]),True),\n",
    "                             StructField(\"ptg_merged_values\",\n",
    "                                         ArrayType(StructType([StructField(\"ptg_year_week\",StringType(),True),\n",
    "                                                               StructField(\"ptg_1\",DoubleType(),True),\n",
    "                                                               StructField(\"ptg_2\",DoubleType(),True),\n",
    "                                                               StructField(\"ptg_3\",DoubleType(),True),\n",
    "                                                               StructField(\"ptg_4\",DoubleType(),True),\n",
    "                                                               StructField(\"ptg_5\",DoubleType(),True),\n",
    "                                                               StructField(\"ptg_6\",DoubleType(),True),\n",
    "                                                               StructField(\"ptg_7\",DoubleType(),True)]),True),\n",
    "                                     True)\n",
    "                        ]\n",
    "                       )\n",
    "    ptg_merged_rdd = (ptg_sel_data\n",
    "                         .withColumn(\"key\",struct([col(\"ro_no\"),\n",
    "                                                   col(\"bpr_tpn\")\n",
    "                                                  ]))\n",
    "                         .withColumn(\"value\",array([struct([col(\"ptg_year_week\"),\n",
    "                                                           col(\"ptg_1\"),\n",
    "                                                           col(\"ptg_2\"),\n",
    "                                                           col(\"ptg_3\"),\n",
    "                                                           col(\"ptg_4\"),\n",
    "                                                           col(\"ptg_5\"),\n",
    "                                                           col(\"ptg_6\"),\n",
    "                                                           col(\"ptg_7\")\n",
    "                                                           ])\n",
    "                                                 ])\n",
    "                                    )\n",
    "                         .select(\"key\",\"value\")\n",
    "                         .rdd\n",
    "                         .reduceByKey(lambda v1,v2 : v1 + v2)\n",
    "                         #.toDF([\"ptg_key\",\"ptg_merged_values\"])\n",
    "                    )\n",
    "    ptg_merged_df = sqlContext.createDataFrame(ptg_merged_rdd,ptg_schema)\n",
    "    \n",
    "ptg_merged_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "join_cond3 = [nws_rrt_rst_dst_df.nws_key == ptg_merged_df.ptg_key]\n",
    "\n",
    "nws_rrt_rst_dst_ptg_df = nws_rrt_rst_dst_df.join(ptg_merged_df,join_cond3,\"leftouter\").drop(\"ptg_key\")#.cache()\n",
    "\n",
    "#nws_rrt_rst_dst_ptg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        nws_key|   nws_merged_values|rrt_merged_values|   rst_merged_values|   dst_merged_values|   ptg_merged_values|   dtp_merged_values|\n",
      "+---------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|[2139,50502413]|[[F13DA,W,Y,277.0...|             null|                null|                null|[[201735,14.1,12....|[[201736,14.1,12....|\n",
      "|[2518,54550994]|[[F51CA,A,N,588.8...|             null|                null|                null|                null|[[201746,15.1,13....|\n",
      "|[2564,50965158]|[[F11MA,W,Y,18.77...|             null|[[2017-10-16,2017...|[[2017-10-16,2017...|[[201736,14.7,12....|[[201734,14.1,12....|\n",
      "+---------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_cond4 = [nws_rrt_rst_dst_ptg_df.nws_key == dtp_merged_df.dtp_key]\n",
    "\n",
    "nws_rrt_rst_dst_ptg_dtp_df = nws_rrt_rst_dst_ptg_df.join(dtp_merged_df,join_cond4,\"leftouter\").drop(\"dtp_key\")#.cache()\n",
    "\n",
    "nws_rrt_rst_dst_ptg_dtp_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------------------+-------------+------------------+\n",
      "|swf_aggr_grp_id|swf_sg_cd|swf_wthr_tesco_week|swf_tesco_day|   swf_wthr_factor|\n",
      "+---------------+---------+-------------------+-------------+------------------+\n",
      "|       WTHEIXMN|    G14GD|                 36|            6|0.9998823404312134|\n",
      "|       WTHWAXM |    G41LD|                 38|            1| 1.116187572479248|\n",
      "|       WTHMINM |    F11BE|                 37|            5|0.8947426080703735|\n",
      "+---------------+---------+-------------------+-------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "swf_sel_data = swf_data.select(col(\"AGGR_GRP_ID\").alias(\"swf_aggr_grp_id\").cast(StringType()),\n",
    "                               col(\"PRODUCT_SUB_GROUP_CODE\").alias(\"swf_sg_cd\").cast(StringType()),\n",
    "                               col(\"WTHR_TESCO_WEEK\").alias(\"swf_wthr_tesco_week\").cast(StringType()),\n",
    "                               col(\"TESCO_DAY\").alias(\"swf_tesco_day\").cast(StringType()),\n",
    "                               col(\"WTHR_FACTOR\").alias(\"swf_wthr_factor\").cast(DoubleType())\n",
    "                               )\n",
    "swf_sel_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "join_cond7 = [nws_sel_data.aggr_grp_id == swf_sel_data.swf_aggr_grp_id, nws_sel_data.sg_cd == swf_sel_data.swf_sg_cd]\n",
    "swf_sel_data = nws_sel_data.join(swf_sel_data,join_cond7).select('ro_no','bpr_tpn','swf_wthr_tesco_week','swf_tesco_day','swf_wthr_factor')\n",
    "#swf_sel_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|        swf_key|   swf_merged_values|\n",
      "+---------------+--------------------+\n",
      "|[5099,51252015]|[[36,3,1.0], [35,...|\n",
      "|[2920,81003974]|[[36,6,0.95935678...|\n",
      "|[2129,68086237]|[[35,3,1.00130093...|\n",
      "+---------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if swf_sel_data.rdd.isEmpty():\n",
    "    schema=StructType([StructField(\"ro_no\", IntegerType(), True), StructField(\"bpr_tpn\", IntegerType(), True), StructField(\"mergedInput\", StringType(), True)])\n",
    "    swf_merged_df = sqlContext.createDataFrame(sc.parallelize([(0,0,'0')]),schema=schema)\n",
    "    swf_merged_rdd = swf_merged_df.rdd.map(lambda row : ((row.swf_ro_no,row.swf_bpr_tpn),row.mergedInput))\n",
    "    swf_merged_df = swf_merged_rdd.toDF([\"swf_key\",\"swf_merged_values\"])\n",
    "else:    \n",
    "    if swf_sel_data.rdd.getNumPartitions() < 400:\n",
    "        swf_sel_data  = swf_sel_data.repartition(400)\n",
    "    \n",
    "    swf_schema = StructType([StructField(\"swf_key\",StructType([StructField(\"ro_no\",IntegerType(),True),\n",
    "                                                               StructField(\"bpr_tpn\",IntegerType(),True)]),True),\n",
    "                             StructField(\"swf_merged_values\",\n",
    "                                         ArrayType(StructType([StructField(\"swf_wthr_tesco_week\",StringType(),True),\n",
    "                                                               StructField(\"swf_tesco_day\",StringType(),True),\n",
    "                                                               StructField(\"swf_wthr_factor\",DoubleType(),True)]),True),\n",
    "                                     True)\n",
    "                        ]\n",
    "                       )\n",
    "\n",
    "    swf_merged_rdd = (swf_sel_data\n",
    "                         .withColumn(\"key\",struct([col(\"ro_no\"),\n",
    "                                                   col(\"bpr_tpn\")\n",
    "                                                  ]))\n",
    "                         .withColumn(\"value\",array([struct([col(\"swf_wthr_tesco_week\"),\n",
    "                                                           col(\"swf_tesco_day\"),\n",
    "                                                           col(\"swf_wthr_factor\")\n",
    "                                                           ])\n",
    "                                                 ])\n",
    "                                    )\n",
    "                         .select(\"key\",\"value\")\n",
    "                         .rdd\n",
    "                         .reduceByKey(lambda v1,v2 : v1 + v2)\n",
    "                         #.toDF([\"swf_key\",\"swf_merged_values\"])\n",
    "                    )\n",
    "    swf_merged_df = sqlContext.createDataFrame(swf_merged_rdd,swf_schema)\n",
    "\n",
    "swf_merged_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        nws_key|   nws_merged_values|rrt_merged_values|   rst_merged_values|   dst_merged_values|   ptg_merged_values|   dtp_merged_values|   swf_merged_values|\n",
      "+---------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|[2139,50502413]|[[F13DA,W,Y,277.0...|             null|                null|                null|[[201735,14.1,12....|[[201736,14.1,12....|[[43,3,0.77321857...|\n",
      "|[2518,54550994]|[[F51CA,A,N,588.8...|             null|                null|                null|                null|[[201746,15.1,13....|[[42,3,1.0], [42,...|\n",
      "|[2564,50965158]|[[F11MA,W,Y,18.77...|             null|[[2017-10-16,2017...|[[2017-10-16,2017...|[[201736,14.7,12....|[[201734,14.1,12....|[[40,3,0.83278703...|\n",
      "+---------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_cond5 = [nws_rrt_rst_dst_ptg_dtp_df.nws_key == swf_merged_df.swf_key]\n",
    "\n",
    "nws_rrt_rst_dst_ptg_dtp_swf_df = nws_rrt_rst_dst_ptg_dtp_df.join(swf_merged_df,join_cond5,\"leftouter\").drop(\"swf_key\")\n",
    "\n",
    "nws_rrt_rst_dst_ptg_dtp_swf_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wef_sel_data = wef_data.select(col(\"AGGR_GRP_ID\").alias(\"wef_aggr_grp_id\").cast(StringType()),\n",
    "                               col(\"PRODUCT_SUB_GROUP_CODE\").alias(\"wef_sg_cd\").cast(StringType()),\n",
    "                               col(\"SALES_DATE\").alias(\"wef_sales_date\").cast(StringType()),\n",
    "                               col(\"WTHR_FACTOR\").alias(\"wef_wthr_factor\").cast(DoubleType())\n",
    "                               )\n",
    "#wef_sel_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------+------------------+\n",
      "|ro_no| bpr_tpn|wef_sales_date|   wef_wthr_factor|\n",
      "+-----+--------+--------------+------------------+\n",
      "| 2547|50935010|    2017-09-20|1.0216832160949707|\n",
      "| 2547|50935010|    2017-09-13|               1.0|\n",
      "| 2547|50935010|    2017-09-19|1.0241930484771729|\n",
      "+-----+--------+--------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_cond8 = [nws_sel_data.aggr_grp_id == wef_sel_data.wef_aggr_grp_id, nws_sel_data.sg_cd == wef_sel_data.wef_sg_cd]\n",
    "wef_sel_data = nws_sel_data.join(wef_sel_data,join_cond8).select('ro_no','bpr_tpn','wef_sales_date','wef_wthr_factor')\n",
    "wef_sel_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|        wef_key|   wef_merged_values|\n",
      "+---------------+--------------------+\n",
      "|[2920,81003974]|[[2017-10-10,1.00...|\n",
      "|[6057,57477477]|[[2017-11-10,0.92...|\n",
      "|[5157,57755937]|[[2017-10-14,1.07...|\n",
      "+---------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if wef_sel_data.rdd.isEmpty():\n",
    "    schema=StructType([StructField(\"ro_no\", IntegerType(), True), StructField(\"bpr_tpn\", IntegerType(), True), StructField(\"mergedInput\", StringType(), True)])\n",
    "    wef_merged_df = sqlContext.createDataFrame(sc.parallelize([(0,0,'0')]),schema=schema)\n",
    "    wef_merged_rdd = wef_merged_df.rdd.map(lambda row : ((row.wef_ro_no,row.wef_bpr_tpn),row.mergedInput))\n",
    "    wef_merged_df = wef_merged_rdd.toDF([\"wef_key\",\"wef_merged_values\"])\n",
    "else:\n",
    "    if wef_sel_data.rdd.getNumPartitions() < 400:\n",
    "        wef_sel_data  = wef_sel_data.repartition(400)\n",
    "    \n",
    "    wef_schema = StructType([StructField(\"wef_key\",StructType([StructField(\"ro_no\",IntegerType(),True),\n",
    "                                                               StructField(\"bpr_tpn\",IntegerType(),True)]),True),\n",
    "                             StructField(\"wef_merged_values\",\n",
    "                                         ArrayType(StructType([StructField(\"wef_sales_date\",StringType(),True),\n",
    "                                                               StructField(\"wef_wthr_factor\",DoubleType(),True)]),True),\n",
    "                                     True)\n",
    "                        ]\n",
    "                       )\n",
    "    \n",
    "    wef_merged_rdd = (wef_sel_data\n",
    "                         .withColumn(\"key\",struct([col(\"ro_no\"),\n",
    "                                                   col(\"bpr_tpn\")\n",
    "                                                  ]))\n",
    "                         .withColumn(\"value\",array([struct([col(\"wef_sales_date\"),\n",
    "                                                           col(\"wef_wthr_factor\")\n",
    "                                                           ])\n",
    "                                                 ])\n",
    "                                    )\n",
    "                         .select(\"key\",\"value\")\n",
    "                         .rdd\n",
    "                         .reduceByKey(lambda v1,v2 : v1 + v2)\n",
    "                         #.toDF([\"wef_key\",\"wef_merged_values\"])\n",
    "                    )\n",
    "    wef_merged_df = sqlContext.createDataFrame(wef_merged_rdd,wef_schema)\n",
    "\n",
    "wef_merged_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        nws_key|   nws_merged_values|rrt_merged_values|   rst_merged_values|   dst_merged_values|   ptg_merged_values|   dtp_merged_values|   swf_merged_values|   wef_merged_values|\n",
      "+---------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|[2139,50502413]|[[F13DA,W,Y,277.0...|             null|                null|                null|[[201735,14.1,12....|[[201736,14.1,12....|[[34,1,0.89153015...|[[2017-10-21,0.95...|\n",
      "|[2518,54550994]|[[F51CA,A,N,588.8...|             null|                null|                null|                null|[[201746,15.1,13....|[[42,3,1.0], [42,...|                null|\n",
      "|[2564,50965158]|[[F11MA,W,Y,18.77...|             null|[[2017-10-16,2017...|[[2017-10-16,2017...|[[201736,14.7,12....|[[201734,14.1,12....|[[35,2,0.94845402...|[[2017-09-20,1.02...|\n",
      "+---------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_cond6 = [nws_rrt_rst_dst_ptg_dtp_swf_df.nws_key == wef_merged_df.wef_key]\n",
    "nws_rrt_rst_dst_ptg_dtp_swf_wef_df = nws_rrt_rst_dst_ptg_dtp_swf_df.join(wef_merged_df,join_cond6,\"leftouter\").drop(\"wef_key\")#.persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "nws_rrt_rst_dst_ptg_dtp_swf_wef_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nws_rrt_rst_dst_ptg_dtp_swf_wef_df.write.format('orc').save(path='hdfs://pphdp/insight_labs/rdf/temp/gct8/intermediate_output_allStores_200by4_'+sub_group+'_'+current_date,mode='overwrite')\n",
    "nws_rrt_rst_dst_ptg_dtp_swf_wef_df.write.format('orc').save(path='hdfs://pphdp/insight_labs/rdf/output/cr_forecast/intermediate_output_allStores_'+sub_group+'_'+current_date+'_'+'merged_version',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-16 06:25:01.528909\n"
     ]
    }
   ],
   "source": [
    "print datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType, DoubleType, IntegerType, StringType, StructField, StructType, DecimalType\n",
    "\n",
    "def forecast_data_schema():\n",
    "    return StructType([\n",
    "        StructField(\"Retail_Outlet_Number\", IntegerType(), True),\n",
    "        StructField(\"Base_Product_Number\", IntegerType(), True),\n",
    "        StructField(\"Run_Date\", StringType(), True),\n",
    "        StructField(\"Sub_Group_Code\", StringType(), True),\n",
    "        StructField(\"Forecast_Date\", StringType(), True),\n",
    "        StructField(\"Forecast_NWS\", DoubleType(), True),\n",
    "        StructField(\"NWS_Stat\", StringType(), True),\n",
    "        StructField(\"Daily_PTP\", DoubleType(), True),\n",
    "        StructField(\"Weather_Factor\", DoubleType(), True),\n",
    "        StructField(\"Expected_Daily_Sales\", DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### These functions are meant for converting the numpy arrays to pandas dataframes for the ease of accessing the same\n",
    "def getBaseAttributes(nws_list):\n",
    "    nws_pd = pd.DataFrame(data=nws_list,\n",
    "                          columns=['sg_cd','nws_stat','step_ind','final_nws','saved_final_nws','aggr_grp_id','stkd_prod_stdt','stkd_prod_endt']\n",
    "                         )\n",
    "    return nws_pd\n",
    "    \n",
    "def getResetAttributes(rrt_list):\n",
    "    rrt_pd = pd.DataFrame(data=rrt_list,\n",
    "                          columns=['rrt_step_eff_dt','rrt_step_type_num','rrt_step_uplift','rrt_step_volume']\n",
    "                         )\n",
    "    rr,rc = rrt_pd.shape\n",
    "    rrt_exists = True if rr > 0 else False\n",
    "    return rr, rc, rrt_pd, rrt_exists\n",
    "\n",
    "def getStepAttributes(rst_list):\n",
    "    rst_pd = pd.DataFrame(data=rst_list,\n",
    "                          columns=['rst_step_eff_dt','rst_step_end_dt','rst_step_type_num','rst_step_id','rst_underlying_step_id','rst_step_uplift','rst_step_volume','rst_actual_nws','rst_wthr_sens_flag','rst_continue_nws']\n",
    "                         )\n",
    "    sr,sc = rst_pd.shape\n",
    "    rst_exists = True if sr > 0 else False\n",
    "    return sr, sc, rst_pd, rst_exists\n",
    "\n",
    "def getDailyAttributes(dst_list):\n",
    "    dst_pd = pd.DataFrame(data=dst_list,\n",
    "                          columns=['dst_start_date','dst_end_date','dst_step_id','dst_underlying_step_id','dst_stack_index']\n",
    "                         )\n",
    "    dr,dc = dst_pd.shape\n",
    "    dst_exists = True if dr > 0 else False\n",
    "    return dr, dc, dst_pd, dst_exists\n",
    "\n",
    "def getEventAttributes(ptg_list):\n",
    "    ptg_pd = pd.DataFrame(data=ptg_list,\n",
    "                          columns=['ptg_year_week','ptg_1','ptg_2','ptg_3','ptg_4','ptg_5','ptg_6','ptg_7']\n",
    "                         )\n",
    "    gr,gc = ptg_pd.shape\n",
    "    ptg_exists = True if gr > 0 else False\n",
    "    return gr, gc, ptg_pd, ptg_exists\n",
    "\n",
    "def getTradingAttributes(dtp_list):\n",
    "    dtp_pd = pd.DataFrame(data=dtp_list,\n",
    "                          columns=['ptp_year_week','ptp_1','ptp_2','ptp_3','ptp_4','ptp_5','ptp_6','ptp_7']\n",
    "                         )\n",
    "    pr,pc = dtp_pd.shape\n",
    "    dtp_exists = True if pr > 0 else False\n",
    "    return pr, pc, dtp_pd, dtp_exists\n",
    "\n",
    "def getSeasonalAttributes(swf_list):\n",
    "    swf_pd = pd.DataFrame(data=swf_list,\n",
    "                          columns=['swf_wthr_tesco_week','swf_tesco_day','swf_wthr_factor']\n",
    "                         )\n",
    "    fr,fc = swf_pd.shape\n",
    "    swf_exists = True if fr > 0 else False\n",
    "    return fr, fc, swf_pd, swf_exists\n",
    "\n",
    "def getWeatherAttributes(wef_list):\n",
    "    wef_pd = pd.DataFrame(data=wef_list,\n",
    "                          columns=['wef_sales_date','wef_wthr_factor']\n",
    "                         )\n",
    "    wr,wc = wef_pd.shape\n",
    "    wef_exists = True if wr > 0 else False\n",
    "    return wr, wc, wef_pd, wef_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### These functions return the PTG and PTP based on year week that is passed\n",
    "\n",
    "def getDaysPtg(gr,ptg_pd,year_week):\n",
    "    for p in np.where(ptg_pd.ptg_year_week == year_week[0])[0]:\n",
    "        days_ptp = eval(\"ptg_pd.ptg_\"+str(year_week[2])+\"[p]\")\n",
    "        return (days_ptp,True)\n",
    "    return (0,False)\n",
    "        \n",
    "def getDaysPtp(pr,dtp_pd,year_week):\n",
    "    for q in np.where(dtp_pd.ptp_year_week == year_week[0])[0]:\n",
    "        days_ptp = eval(\"dtp_pd.ptp_\"+str(year_week[2])+\"[q]\")\n",
    "        return days_ptp\n",
    "    return 14.3\n",
    "\n",
    "# This function returns the Seasonal Weather Factor for a day\n",
    "def getDaysSeasonality(fr,swf_pd,year_week):\n",
    "    for r in np.where((year_week[1] == swf_pd.swf_wthr_tesco_week) & (year_week[2] == swf_pd.swf_tesco_day))[0]:\n",
    "        return swf_pd.swf_wthr_factor[r]\n",
    "    return 1.0\n",
    "\n",
    "# This function returns the Real Weather Factor for a day\n",
    "def getRealWF(day_no,wef_pd,wr,swf_pd,fr,year_week):\n",
    "    iso_date = day_no.strftime(\"%Y-%m-%d\")\n",
    "    for d in np.where(wef_pd.wef_sales_date == iso_date)[0]:\n",
    "        return wef_pd.wef_wthr_factor[d]\n",
    "    return getDaysSeasonality(fr,swf_pd,year_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function is meant to return the actual NWS from a back to back promotion\n",
    "def getB2BPromo(rst_pd,sr,date):\n",
    "    for b in range (sr):\n",
    "        if date == rst_pd.rst_step_end_dt[b] \\\n",
    "        and rst_pd.rst_step_type_num[b] in [201,202,203,204]:\n",
    "            if pd.notnull(rst_pd.rst_actual_nws[b]):\n",
    "                return (rst_pd.rst_actual_nws[b],True)\n",
    "    \n",
    "    return (0,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function return the range of seven days from the day that is passed to it.\n",
    "def getFutureDays(step_eff_dt,date_limit):\n",
    "    step_date_plus_6 = step_eff_dt + timedelta(days=6)\n",
    "    futureDays = []\n",
    "    for key in sorted(date_limit.keys()):\n",
    "        if key >= step_eff_dt and key <= step_date_plus_6:\n",
    "            futureDays.append(date_limit[key])\n",
    "    return futureDays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function calculates the Seasonal Weather Factor for the range of dates that is passed.\n",
    "def getCalcSeasonalWF(range_of_dates,ptg_pd,ptg_exists,gr,gc,dtp_pd,pr,pc,dtp_exists,swf_pd,swf_exists,fr,fc):\n",
    "    ptp = []\n",
    "    swf = []\n",
    "    ptg_found = False\n",
    "    sum_of_ptps = 0.0\n",
    "    sum_of_ptps_swf = 0.0\n",
    "    \n",
    "    for c in range (len(range_of_dates)):\n",
    "        days_ptp = 0.142\n",
    "        if ptg_exists:\n",
    "            # Use PTG, if available\n",
    "            (days_ptp,ptg_found) = getDaysPtg(gr,ptg_pd,range_of_dates[c])\n",
    "        \n",
    "        if not ptg_found:\n",
    "            if dtp_exists:\n",
    "                # If there exists no PTG, use PTP\n",
    "                days_ptp = getDaysPtp(pr,dtp_pd,range_of_dates[c])\n",
    "        ptp.append(days_ptp)\n",
    "        \n",
    "        days_swf = getDaysSeasonality(fr,swf_pd,range_of_dates[c])\n",
    "        swf.append(days_swf)\n",
    "            \n",
    "    swf_into_ptp = [s * p for s,p in zip(ptp,swf)]\n",
    "    \n",
    "    sum_of_ptps_swf = reduce(lambda x,y : x+y ,swf_into_ptp)\n",
    "    sum_of_ptps = reduce(lambda x,y : x+y ,ptp)\n",
    "    \n",
    "    if sum_of_ptps > 0:\n",
    "        return sum_of_ptps_swf/sum_of_ptps\n",
    "    else:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function calculates the Real Weather Factor for the range of dates that is passed.\n",
    "def getCalcRealWF(step_eff_dt,date_limit,ptg_pd,ptg_exists,gr,gc,dtp_pd,dtp_exists,pr,pc,\n",
    "                  wef_pd,wef_exists,wr,wc,swf_pd,swf_exists,fr,fc):\n",
    "    ptp = []\n",
    "    wef = []\n",
    "    ptg_found = False\n",
    "    sum_of_ptps = 0.0\n",
    "    sum_of_ptps_wef = 0.0\n",
    "    \n",
    "    step_date_plus_6 = step_eff_dt + timedelta(days=6)\n",
    "    range_of_dates = []\n",
    "    iso_dates = []\n",
    "    \n",
    "    for key in sorted(date_limit.keys()):\n",
    "        if key >= step_eff_dt and key <= step_date_plus_6:\n",
    "            iso_dates.append(key)\n",
    "            range_of_dates.append(date_limit[key])\n",
    "                    \n",
    "    for c in range (len(range_of_dates)):\n",
    "        days_ptp = 0.143\n",
    "        if ptg_exists:\n",
    "            # Use PTG, if available\n",
    "            (days_ptp,ptg_found) = getDaysPtg(gr,ptg_pd,range_of_dates[c])\n",
    "        \n",
    "        if not ptg_found:\n",
    "            if dtp_exists:\n",
    "                # If there exists no PTG, use PTP\n",
    "                days_ptp = getDaysPtp(pr,dtp_pd,range_of_dates[c])\n",
    "        ptp.append(days_ptp)\n",
    "        days_wef = getRealWF(iso_dates[c],wef_pd,wr,swf_pd,fr,range_of_dates[c])\n",
    "        wef.append(days_wef)\n",
    "            \n",
    "    wef_into_ptp = [s * p for s,p in zip(ptp,wef)]\n",
    "    \n",
    "    sum_of_ptps_wef = reduce(lambda x,y : x+y ,wef_into_ptp)\n",
    "    sum_of_ptps = reduce(lambda x,y : x+y ,ptp)\n",
    "    \n",
    "    if sum_of_ptps > 0:\n",
    "        return sum_of_ptps_wef/sum_of_ptps\n",
    "    else:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function accepts all the attributes that influence the forecast for the store/product as one record \n",
    "# and returns the forecast for 'n' number of days\n",
    "def getForecast(row):\n",
    "    dst_list = np.array(row.dst_merged_values)    \n",
    "    ptg_exists = False\n",
    "    rrt_exists = False\n",
    "    rst_exists = False\n",
    "    dst_exists = False\n",
    "    dtp_exists = False\n",
    "    swf_exists = False\n",
    "    wef_exists = False\n",
    "    \n",
    "    # Get the broadcated calendar table details\n",
    "    date_range = cal_bc_var1.value\n",
    "    swf_range = cal_bc_var2.value\n",
    "    current_date = (datetime.now()+ timedelta(days=-1)).strftime(\"%Y-%m-%d\")\n",
    "    #current_date = '2017-08-16'\n",
    "    \n",
    "    # Lets convert the infleuncers into pd dataframe for further processing\n",
    "    nws_pd = getBaseAttributes(row.nws_merged_values)\n",
    "    \n",
    "    (rr,rc,rrt_pd,rrt_exists) = getResetAttributes(row.rrt_merged_values if row.rrt_merged_values is not None else [])\n",
    "    (sr,sc,rst_pd,rst_exists) = getStepAttributes(row.rst_merged_values if row.rst_merged_values is not None else [])\n",
    "    (pr,pc,dtp_pd,dtp_exists) = getTradingAttributes(row.dtp_merged_values if row.dtp_merged_values is not None else [])\n",
    "    (gr,gc,ptg_pd,ptg_exists) = getEventAttributes(row.ptg_merged_values if row.ptg_merged_values is not None else [])\n",
    "    (fr,fc,swf_pd,swf_exists) = getSeasonalAttributes(row.swf_merged_values if row.swf_merged_values is not None else [])\n",
    "    (wr,wc,wef_pd,wef_exists) = getWeatherAttributes(row.wef_merged_values if row.wef_merged_values is not None else [])\n",
    "    \n",
    "    if row.dst_merged_values is not None:\n",
    "        dr,dc = dst_list.shape\n",
    "        dst_exists = True      \n",
    "        \n",
    "    ###############################################################################\n",
    "    # Start of forecast algorithm - For every day in the forecast window\n",
    "    # 1. Apply volume/percentage reset to the base NWS\n",
    "    # 2. If there is any step underlying the reset, reset it's volume and stepped nws accordingly\n",
    "    # 3. Apply the effect of each step in the ascending order of their stack index \n",
    "    \n",
    "    forecast_data = []\n",
    "    forecast_data.extend([row.nws_key.ro_no,row.nws_key.bpr_tpn])\n",
    "    \n",
    "    # Get the appropriate NWS to start with : Use saved if available, else final NWS\n",
    "    if pd.notnull(nws_pd.saved_final_nws[0]):\n",
    "        base_nws = nws_pd.saved_final_nws[0]\n",
    "        base_nws_reset = nws_pd.saved_final_nws[0]\n",
    "    else:\n",
    "        base_nws = nws_pd.final_nws[0]\n",
    "        base_nws_reset = nws_pd.final_nws[0]\n",
    "    \n",
    "    # In case stocked start or end date is null, set it to maximum value\n",
    "    maximum_date = datetime.strptime('99991231', \"%Y%m%d\").date()\n",
    "    if pd.isnull(nws_pd.stkd_prod_stdt[0]):\n",
    "        nws_pd.stkd_prod_stdt[0] = maximum_date\n",
    "    \n",
    "    if pd.isnull(nws_pd.stkd_prod_endt[0]):\n",
    "        nws_pd.stkd_prod_endt[0] = maximum_date\n",
    "    \n",
    "    # For every day in the forecast window, loop through RRT, RST, DST to arrive at forecast NWS\n",
    "    for day in sorted(date_range.keys()):\n",
    "        forecast_date = day\n",
    "        if forecast_date >= nws_pd.stkd_prod_stdt[0] \\\n",
    "        and forecast_date <= nws_pd.stkd_prod_endt[0]: \n",
    "            vol_reset = False\n",
    "            per_reset = False\n",
    "            ptg_found = False\n",
    "            ptp_found = False\n",
    "            b2b_promo_found = False\n",
    "            base_nws = base_nws_reset\n",
    "            total_reset_factor = 1\n",
    "            reset_factor = 1\n",
    "            \n",
    "            #See if there are any resets for the forecast day. If so, reset the base accordingly\n",
    "            if rrt_exists:\n",
    "                for j in np.where(rrt_pd.rrt_step_eff_dt == forecast_date)[0]:                \n",
    "                    if rrt_pd.rrt_step_type_num[j] in [252,452]:\n",
    "                        if nws_pd.nws_stat[0] == 'W' and swf_exists:\n",
    "                            current_plus_7days = getFutureDays(rrt_pd.rrt_step_eff_dt[j],swf_range)\n",
    "                            seasonal_wf = getCalcSeasonalWF(current_plus_7days,\n",
    "                                                            ptg_pd,ptg_exists,gr,gc,\n",
    "                                                            dtp_pd,dtp_exists,pr,pc,\n",
    "                                                            swf_pd,swf_exists,fr,fc)\n",
    "                        else:\n",
    "                            seasonal_wf = 1.0\n",
    "                        base_nws = float(rrt_pd.rrt_step_volume[j]) / seasonal_wf\n",
    "                        if rrt_pd.rrt_step_type_num[j] == 252:\n",
    "                            vol_reset = True\n",
    "                    \n",
    "                    if rrt_pd.rrt_step_type_num[j] in [251,451,351]:\n",
    "                        reset_factor = 1 + rrt_pd.rrt_step_uplift[j] / 100.0\n",
    "                        total_reset_factor = total_reset_factor * reset_factor\n",
    "                        if rrt_pd.rrt_step_type_num[j] == 251:\n",
    "                            per_reset = True\n",
    "\n",
    "            base_nws = base_nws * total_reset_factor\n",
    "            base_nws_reset = base_nws  \n",
    "            \n",
    "            if rst_exists and dst_exists:\n",
    "                # Now that the base is reset, we need to update steps if any, with the same reset factor\n",
    "                for k in  np.where((rst_pd.rst_step_eff_dt < forecast_date) \n",
    "                                   & (rst_pd.rst_step_end_dt >= forecast_date))[0]:\n",
    "                    if vol_reset:\n",
    "                        rst_pd.rst_actual_nws[k] = base_nws\n",
    "                    if per_reset:\n",
    "                        if pd.notnull(rst_pd.rst_actual_nws[k]):\n",
    "                            rst_pd.rst_actual_nws[k] = float(rst_pd.rst_actual_nws[k]) * total_reset_factor\n",
    "\n",
    "\n",
    "                dst1_list =  dst_list[ (dst_list[:,0] <= forecast_date) & ( dst_list[:,1] >= forecast_date)]\n",
    "                dst1_list = dst1_list[dst1_list[:,4].argsort()]\n",
    "                \n",
    "                if dst1_list.size > 0:\n",
    "                    (tr,tc,dst1_pd,dst_exists) = getDailyAttributes(dst1_list if dst1_list is not None else [])\n",
    "                    for m in range(tr):\n",
    "                        for n in np.where((dst1_pd.dst_step_id[m] == rst_pd.rst_step_id) &\n",
    "                                          (dst1_pd.dst_underlying_step_id[m] == rst_pd.rst_underlying_step_id))[0]:\n",
    "                            if pd.notnull(rst_pd.rst_actual_nws[n]):\n",
    "                                base_nws = rst_pd.rst_actual_nws[n]\n",
    "                                rst_pd.rst_actual_nws[n] = base_nws\n",
    "                                \n",
    "                            else:\n",
    "                                if pd.notnull(rst_pd.rst_step_volume[n]):\n",
    "                                    if rst_pd.rst_continue_nws[n] == 'Y':\n",
    "                                        prev_promo_end_dt = rst_pd.rst_step_eff_dt[n] + timedelta(days=-1)\n",
    "                                        (base_nws,b2b_promo_found) = getB2BPromo(rst_pd,sr,prev_promo_end_dt)\n",
    "                                    if not b2b_promo_found:\n",
    "                                        seasonal_wf = 1.0\n",
    "                                        if nws_pd.nws_stat[0] == 'W':\n",
    "                                            if rst_pd.rst_wthr_sens_flag[n] in ['Y','A'] and swf_exists:\n",
    "                                                current_plus_7days = getFutureDays(rst_pd.rst_step_eff_dt[n],swf_range)\n",
    "                                                seasonal_wf = getCalcSeasonalWF(current_plus_7days,\n",
    "                                                                                ptg_pd,ptg_exists,gr,gc,\n",
    "                                                                                dtp_pd,dtp_exists,pr,pc,\n",
    "                                                                                swf_pd,swf_exists,fr,fc)\n",
    "                                            if rst_pd.rst_wthr_sens_flag[n] == 'N' and wef_exists:\n",
    "                                                seasonal_wf = getCalcRealWF(rst_pd.rst_step_eff_dt[n],swf_range,\n",
    "                                                                            ptg_pd,ptg_exists,gr,gc,\n",
    "                                                                            dtp_pd,dtp_exists,pr,pc,\n",
    "                                                                            wef_pd,wef_exists,wr,wc,\n",
    "                                                                            swf_pd,swf_exists,fr,fc)\n",
    "                                        base_nws = rst_pd.rst_step_volume[n] / seasonal_wf\n",
    "                                    if pd.isnull(rst_pd.rst_actual_nws[n]):\n",
    "                                        rst_pd.rst_actual_nws[n] = base_nws\n",
    "                                else:\n",
    "                                    if pd.notnull(rst_pd.rst_step_uplift[n]):\n",
    "                                        base_nws = base_nws * (1+(rst_pd.rst_step_uplift[n])/100.0)        \n",
    "                                        \n",
    "\n",
    "            # Now that we have the forecast nws, we will find the respective ptp/ptg that needs to be applied\n",
    "            # use the respective year week from the date_range broadcasted value\n",
    "            \n",
    "            days_ptp = 14.3    # default PTP, in case we end up not finding PTP/PTG entries\n",
    "            if ptg_exists:\n",
    "                # Use PTG, if available\n",
    "                (days_ptp,ptg_found) = getDaysPtg(gr,ptg_pd,date_range[day])\n",
    "            \n",
    "            if not ptg_found:\n",
    "                if dtp_exists:\n",
    "                    # If there exists no PTG, use PTP\n",
    "                    days_ptp = getDaysPtp(pr,dtp_pd,date_range[day])\n",
    "            \n",
    "            # Derive the weather factors for the weather products\n",
    "            if nws_pd.nws_stat[0] == 'W' and wef_exists:\n",
    "                days_wthr_factor = getRealWF(forecast_date,wef_pd,wr,swf_pd,fr,date_range[day])\n",
    "            else:\n",
    "                days_wthr_factor = 1.0\n",
    "\n",
    "            exp_daily_sls = np.around(base_nws*(days_ptp/100.0)*days_wthr_factor,2)\n",
    "\n",
    "            # The EDS for the day along with the respective attributes needs to appended to output list\n",
    "            # Note that Pyspark DF's doesn't support numpy float type. Hence there is a need to convert them to float\n",
    "            # before returning\n",
    "            forecasted_date = forecast_date.strftime(\"%Y-%m-%d\")\n",
    "            forecast_data.extend([current_date,nws_pd.sg_cd[0],forecasted_date,float(base_nws),nws_pd.nws_stat[0],\n",
    "                                  float(np.around(days_ptp,2)),float(days_wthr_factor),float(exp_daily_sls)]) \n",
    "            \n",
    "        else:\n",
    "            # The product is not stocked, default the forecast to zeros.\n",
    "            forecasted_date = forecast_date.strftime(\"%Y-%m-%d\")\n",
    "            forecast_data.extend([current_date,nws_pd.sg_cd[0],forecasted_date,0.0,' ',0.0,1.0,0.0]) \n",
    "    return forecast_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:33.844580\n"
     ]
    }
   ],
   "source": [
    "# Mid-term forecast Runner\n",
    "t1_comp = datetime.now()\n",
    "\n",
    "#intermediate_output = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/output/cr_forecast/intermediate_output_allStores_F11MA_2017-08-02_merged_version').filter('nws_key.ro_no=5777').filter('nws_key.bpr_tpn=50935010')\n",
    "#intermediate_output = intermediate_output.repartition(200000)\n",
    "intermediate_output = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/output/cr_forecast/intermediate_output_allStores_F11MA_2017-08-02_merged_version').filter('nws_key.ro_no=4483').filter('nws_key.bpr_tpn=50935010')\n",
    "\n",
    "# Broadcast the CAL date range required for get_forecast function.\n",
    "cal_sel_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/output/cr_forecast/cal_bc_set1_'+sub_group)\n",
    "cal_list1 = cal_sel_data.rdd.map(lambda x : (x.calendar_date,(x.year_week,x.week_number,x.day_number))).collectAsMap()\n",
    "cal_bc_var1 = sc.broadcast(cal_list1)\n",
    "\n",
    "cal_sel_data = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/output/cr_forecast/cal_bc_set2_'+sub_group)\n",
    "cal_list2 = cal_sel_data.rdd.map(lambda x : (x.calendar_date,(x.year_week,x.week_number,x.day_number))).collectAsMap()\n",
    "cal_bc_var2 = sc.broadcast(cal_list2)\n",
    "\n",
    "\n",
    "forecast_rdd = intermediate_output.map(getForecast)\n",
    "#forecast_rdd.take(10)\n",
    "\n",
    "# # This is meant to de-normalise the output\n",
    "number_of_columns = 8 # number of columns for each store/product\n",
    "starting_point = 1 # in python terms, so the counter starts from 0\n",
    "forecast_at_day = (forecast_rdd.flatMap(lambda x:( ((x[:starting_point+1]) + v) for  v in [x[starting_point:][i:i + number_of_columns] for i in xrange(starting_point, len(x)-starting_point, number_of_columns)]\n",
    "                                )  \n",
    "                      )\n",
    "             .toDF(schema=forecast_data_schema())\n",
    "          )\n",
    "\n",
    "path_name = ('/insight_labs/rdf/output/cr_forecast/final_output_'+sub_group+'_'+current_date+'_'+'merged_version')\n",
    "forecast_at_day.coalesce(2000).write.format(\"orc\").save(path=path_name,mode=\"overwrite\")\n",
    "\n",
    "\n",
    "# print 'has been calculated'\n",
    "t2_comp = datetime.now()\n",
    "print t2_comp - t1_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#intermediate_output = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/output/cr_forecast/intermediate_output_allStores_F11MA_2017-08-02_merged_version')\n",
    "intermediate_output = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/output/cr_forecast/intermediate_output_allStores_'+sub_group+'_'+current_date+'_'+'merged_version')\n",
    "#.limit(18000000)\n",
    "#intermediate_output = sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/output/cr_forecast/intermediate_output_allStores_allSG_2017-08-08_merged_version').filter('nws_key.ro_no=2002')\n",
    "intermediate_output = intermediate_output.repartition(200000)\n",
    "#.filter('nws_key.ro_no=5288').filter('nws_key.bpr_tpn=50935010')\n",
    "#intermediate_output.cache()\n",
    "#intermediate_output.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://pphdp/insight_labs/rdf/output/cr_forecast/intermediate_output_allStores_F11MA_2017-10-15_merged_version\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path=('hdfs://pphdp/insight_labs/rdf/output/cr_forecast/intermediate_output_allStores_'+sub_group+'_'+current_date+'_'+'merged_version')\n",
    "print path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=sqlContext.read.orc('hdfs://pphdp/insight_labs/rdf/output/cr_forecast/final_output_'+sub_group+'_'+current_date+'_'+'merged_version')\n",
    "#df.show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------+--------------+-------------+------------+--------+---------+------------------+--------------------+\n",
      "|Retail_Outlet_Number|Base_Product_Number|  Run_Date|Sub_Group_Code|Forecast_Date|Forecast_NWS|NWS_Stat|Daily_PTP|    Weather_Factor|Expected_Daily_Sales|\n",
      "+--------------------+-------------------+----------+--------------+-------------+------------+--------+---------+------------------+--------------------+\n",
      "|                4483|           50935010|2017-10-15|         F11MA|   2017-08-18|      119.16|       W|     17.9|1.1597141027450562|               24.74|\n",
      "|                4483|           50935010|2017-10-15|         F11MA|   2017-08-19|      119.16|       W|     18.6|1.1737021207809448|               26.01|\n",
      "|                4483|           50935010|2017-10-15|         F11MA|   2017-08-20|      119.16|       W|     11.1|1.1426117420196533|               15.11|\n",
      "|                4483|           50935010|2017-10-15|         F11MA|   2017-08-21|      119.16|       W|     13.8| 1.225982904434204|               20.16|\n",
      "|                4483|           50935010|2017-10-15|         F11MA|   2017-08-22|      119.16|       W|     10.6| 1.158815622329712|               14.64|\n",
      "|                4483|           50935010|2017-10-15|         F11MA|   2017-08-23|      119.16|       W|     13.6|1.1657111644744873|               18.89|\n",
      "|                4483|           50935010|2017-10-15|         F11MA|   2017-08-24|      119.16|       W|     14.4|1.1613082885742188|               19.93|\n",
      "|                4483|           50935010|2017-10-15|         F11MA|   2017-08-25|      119.16|       W|     17.9|1.1527135372161865|               24.59|\n",
      "|                4483|           50935010|2017-10-15|         F11MA|   2017-08-26|      119.16|       W|     18.6|1.1645569801330566|               25.81|\n",
      "|                4483|           50935010|2017-10-15|         F11MA|   2017-08-27|      119.16|       W|     11.1|1.1342171430587769|                15.0|\n",
      "+--------------------+-------------------+----------+--------------+-------------+------------+--------+---------+------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# url = \"jdbc:teradata://TDPS.ukroi.tesco.org/DATABASE=DXWI_PROD_RDF_PLAY_PEN\"\n",
    "# properties = {\n",
    "#     \"user\": \"xg66\",\n",
    "#     \"password\": \"tesco123\",\n",
    "#      \"driver\" : \"com.teradata.jdbc.TeraDriver\"\n",
    "# }\n",
    "# mode=\"overwrite\"_merged_version\n",
    "#forecast_at_day.coalesce(1).write.jdbc(url=url, table=\"MID_TERM_FORECAST\", mode=mode, properties=properties)\n",
    "## vijaydf.write.format('com.databricks.spark.csv').save(path='/insight_labs/rdf/output/cr_forecast/final_output_'+sub_group+'_CSV_merged_version',mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
